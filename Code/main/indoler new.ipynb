{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce6c4781",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Setup & Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772d3ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install requirements\n",
    "# pip install -r ../../requirements.txt\n",
    "# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf956f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import copy\n",
    "import gc\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# ML libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification\n",
    ")\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Evaluation\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report as seq_classification_report\n",
    "\n",
    "# Visualization\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✅ All libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010e9a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dependencies check:\")\n",
    "\n",
    "try:\n",
    "    import transformers\n",
    "    print(f\"transformers: {transformers.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"transformers not installed\")\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"torch: {torch.__version__}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "except ImportError:\n",
    "    print(\"torch not installed\")\n",
    "\n",
    "try:\n",
    "    import datasets\n",
    "    print(f\"datasets: {datasets.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"datasets not installed\")\n",
    "\n",
    "try:\n",
    "    import evaluate\n",
    "    print(f\"evaluate: {evaluate.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"evaluate not installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa957a24",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88aab89c",
   "metadata": {},
   "source": [
    "# EDA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d9d783",
   "metadata": {},
   "source": [
    "- https://github.com/ir-nlp-csui/indoler\n",
    "- https://putusan3.mahkamahagung.go.id/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7f0611",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with open('../../Datasets/PUBLIC/data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "df = pd.json_normalize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88903f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1ae5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"Shape DataFrame: {df.shape}\")\n",
    "print(f\"\\nColumns DataFrame:\")\n",
    "print(df.columns.tolist())\n",
    "print(f\"\\nInfo DataFrame:\")\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcb88df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_all_tags(df):\n",
    "    all_tags = []\n",
    "    for tags in df['text-tags']:\n",
    "        all_tags.extend(tags)\n",
    "    return Counter(all_tags)\n",
    "\n",
    "# distribusi tags\n",
    "tag_counts = get_all_tags(df)\n",
    "print(\"Distribusi Tags NER:\")\n",
    "for tag, count in tag_counts.most_common():\n",
    "    print(f\"{tag}: {count:,}\")\n",
    "\n",
    "#persentase non-O tags\n",
    "total_tags = sum(tag_counts.values())\n",
    "non_o_tags = sum(count for tag, count in tag_counts.items() if tag != 'O')\n",
    "print(f\"\\nTotal tags: {total_tags:,}\")\n",
    "print(f\"Non-O tags: {non_o_tags:,} ({non_o_tags/total_tags*100:.2f}%)\")\n",
    "print(f\"O tags: {tag_counts['O']:,} ({tag_counts['O']/total_tags*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bc6eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def analyze_entity_types(df):\n",
    "\n",
    "    entity_types = set()\n",
    "    entity_examples = {}\n",
    "    \n",
    "    for idx in range(len(df)):\n",
    "        text = df.iloc[idx]['text']\n",
    "        tags = df.iloc[idx]['text-tags']\n",
    "        \n",
    "        current_entity = []\n",
    "        current_entity_type = None\n",
    "        \n",
    "        for token, tag in zip(text, tags):\n",
    "            if tag.startswith('B-'):\n",
    "                # Simpan entity sebelumnya\n",
    "                if current_entity and current_entity_type:\n",
    "                    entity_text = ' '.join(current_entity)\n",
    "                    if current_entity_type not in entity_examples:\n",
    "                        entity_examples[current_entity_type] = []\n",
    "                    if len(entity_examples[current_entity_type]) < 5: \n",
    "                        entity_examples[current_entity_type].append(entity_text)\n",
    "                \n",
    "                # Mulai entity baru\n",
    "                current_entity_type = tag[2:]\n",
    "                entity_types.add(current_entity_type)\n",
    "                current_entity = [token]\n",
    "            \n",
    "            elif tag.startswith('I-') and current_entity_type == tag[2:]:\n",
    "                current_entity.append(token)\n",
    "            \n",
    "            else:  # tag == 'O' atau inconsistent I-tag\n",
    "        \n",
    "                if current_entity and current_entity_type:\n",
    "                    entity_text = ' '.join(current_entity)\n",
    "                    if current_entity_type not in entity_examples:\n",
    "                        entity_examples[current_entity_type] = []\n",
    "                    if len(entity_examples[current_entity_type]) < 5:\n",
    "                        entity_examples[current_entity_type].append(entity_text)\n",
    "                \n",
    "                current_entity = []\n",
    "                current_entity_type = None\n",
    "        \n",
    "        \n",
    "        if current_entity and current_entity_type:\n",
    "            entity_text = ' '.join(current_entity)\n",
    "            if current_entity_type not in entity_examples:\n",
    "                entity_examples[current_entity_type] = []\n",
    "            if len(entity_examples[current_entity_type]) < 5:\n",
    "                entity_examples[current_entity_type].append(entity_text)\n",
    "    \n",
    "    return entity_types, entity_examples\n",
    "\n",
    "\n",
    "print(\"Menganalisis jenis-jenis entity...\")\n",
    "entity_types, entity_examples = analyze_entity_types(df)\n",
    "\n",
    "print(f\"\\nDitemukan {len(entity_types)} jenis entity:\")\n",
    "for entity_type in sorted(entity_types):\n",
    "    print(f\"\\n• {entity_type}:\")\n",
    "    if entity_type in entity_examples:\n",
    "        for example in entity_examples[entity_type]:\n",
    "            print(f\"  - '{example}'\")\n",
    "    else:\n",
    "        print(f\"  - (tidak ada contoh)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381fe0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def check_annotation_consistency(df):\n",
    "#     errors = []\n",
    "    \n",
    "#     for idx in range(len(df)):\n",
    "#         tags = df.iloc[idx]['text-tags']\n",
    "#         text = df.iloc[idx]['text']\n",
    "        \n",
    "#         for i, tag in enumerate(tags):\n",
    "#             # Check 1: I-tag harus didahului oleh B-tag atau I-tag dengan tipe yang sama\n",
    "#             if tag.startswith('I-'):\n",
    "#                 entity_type = tag[2:]\n",
    "#                 if i == 0:  # I-tag di awal sequence\n",
    "#                     errors.append({\n",
    "#                         'doc_id': idx,\n",
    "#                         'position': i,\n",
    "#                         'error': f'I-tag at beginning: {tag}',\n",
    "#                         'context': ' '.join(text[max(0, i-2):i+3])\n",
    "#                     })\n",
    "#                 else:\n",
    "#                     prev_tag = tags[i-1]\n",
    "#                     if not (prev_tag == f'B-{entity_type}' or prev_tag == f'I-{entity_type}'):\n",
    "#                         errors.append({\n",
    "#                             'doc_id': idx,\n",
    "#                             'position': i,\n",
    "#                             'error': f'I-tag without proper B-tag: {prev_tag} -> {tag}',\n",
    "#                             'context': ' '.join(text[max(0, i-2):i+3])\n",
    "#                         })\n",
    "    \n",
    "#     return errors\n",
    "\n",
    "\n",
    "# print(\"Mengecek konsistensi anotasi BIO...\")\n",
    "# errors = check_annotation_consistency(df)\n",
    "\n",
    "# if errors:\n",
    "#     print(f\"\\nDitemukan {len(errors)} error konsistensi:\")\n",
    "#     for i, error in enumerate(errors[:10]):\n",
    "#         print(f\"{i+1}. Doc {error['doc_id']}, Pos {error['position']}: {error['error']}\")\n",
    "#         print(f\"   Context: '{error['context']}'\")\n",
    "    \n",
    "#     if len(errors) > 10:\n",
    "#         print(f\"\\n... dan {len(errors)-10} error lainnya\")\n",
    "# else:\n",
    "#     print(\"\\nTidak ditemukan error konsistensi BIO!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23edeebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def display_annotations(df, idx, max_tokens=50):\n",
    "    \"\"\"\n",
    "    Menampilkan teks dengan highlight anotasi untuk review manual\n",
    "    \"\"\"\n",
    "    text = df.iloc[idx]['text']\n",
    "    tags = df.iloc[idx]['text-tags']\n",
    "    \n",
    "    print(f\"\\n=== DOCUMENT {idx} (ID: {df.iloc[idx]['id']}) ===\")\n",
    "    print(f\"Verdict: {df.iloc[idx]['verdict']}\")\n",
    "    print(f\"Indictment: {df.iloc[idx]['indictment']}\")\n",
    "    print(f\"Lawyer: {df.iloc[idx]['lawyer']}\")\n",
    "    print(f\"Owner: {df.iloc[idx]['owner']}\")\n",
    "    print(\"\\n--- ANNOTATED TEXT ---\")\n",
    "    \n",
    "  \n",
    "    display_text = text[:max_tokens]\n",
    "    display_tags = tags[:max_tokens]\n",
    "    \n",
    "    current_entity = []\n",
    "    current_entity_type = None\n",
    "    \n",
    "    for i, (token, tag) in enumerate(zip(display_text, display_tags)):\n",
    "        if tag.startswith('B-'):\n",
    "            \n",
    "            if current_entity:\n",
    "                entity_text = ' '.join(current_entity)\n",
    "                print(f\"[{entity_text}]({current_entity_type})\", end=' ')\n",
    "                current_entity = []\n",
    "            \n",
    "            \n",
    "            current_entity_type = tag[2:]\n",
    "            current_entity = [token]\n",
    "        \n",
    "        elif tag.startswith('I-'):\n",
    "    \n",
    "            if current_entity_type == tag[2:]:\n",
    "                current_entity.append(token)\n",
    "            else:\n",
    "       \n",
    "                if current_entity:\n",
    "                    entity_text = ' '.join(current_entity)\n",
    "                    print(f\"[{entity_text}]({current_entity_type})\", end=' ')\n",
    "                current_entity_type = tag[2:]\n",
    "                current_entity = [token]\n",
    "        \n",
    "        else:  # tag == 'O'\n",
    "            \n",
    "            if current_entity:\n",
    "                entity_text = ' '.join(current_entity)\n",
    "                print(f\"[{entity_text}]({current_entity_type})\", end=' ')\n",
    "                current_entity = []\n",
    "                current_entity_type = None\n",
    "            \n",
    "          \n",
    "            print(token, end=' ')\n",
    "    \n",
    "\n",
    "    if current_entity:\n",
    "        entity_text = ' '.join(current_entity)\n",
    "        print(f\"[{entity_text}]({current_entity_type})\", end=' ')\n",
    "    \n",
    "    print(\"\\n\")\n",
    "\n",
    "    entities = []\n",
    "    current_entity = []\n",
    "    current_entity_type = None\n",
    "    \n",
    "    for token, tag in zip(text, tags):\n",
    "        if tag.startswith('B-'):\n",
    "            if current_entity:\n",
    "                entities.append((' '.join(current_entity), current_entity_type))\n",
    "                current_entity = []\n",
    "            current_entity_type = tag[2:]\n",
    "            current_entity = [token]\n",
    "        elif tag.startswith('I-') and current_entity_type == tag[2:]:\n",
    "            current_entity.append(token)\n",
    "        else:\n",
    "            if current_entity:\n",
    "                entities.append((' '.join(current_entity), current_entity_type))\n",
    "                current_entity = []\n",
    "                current_entity_type = None\n",
    "    \n",
    "    if current_entity:\n",
    "        entities.append((' '.join(current_entity), current_entity_type))\n",
    "    \n",
    "    if entities:\n",
    "        print(\"--- EXTRACTED ENTITIES ---\")\n",
    "        for entity, entity_type in entities:\n",
    "            print(f\"• {entity_type}: '{entity}'\")\n",
    "    else:\n",
    "        print(\"--- NO ENTITIES FOUND ---\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "\n",
    "print(\"MANUAL REVIEW - Contoh Anotasi:\")\n",
    "for i in range(min(5, len(df))):\n",
    "    display_annotations(df, i, max_tokens=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eb303c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisasi distribusi entity types\n",
    "def visualize_entity_distribution(df):\n",
    "\n",
    "    # Hitung jumlah entities per type\n",
    "    entity_counts = {}\n",
    "    \n",
    "    for idx in range(len(df)):\n",
    "        text = df.iloc[idx]['text']\n",
    "        tags = df.iloc[idx]['text-tags']\n",
    "        \n",
    "        current_entity_type = None\n",
    "        \n",
    "        for token, tag in zip(text, tags):\n",
    "            if tag.startswith('B-'):\n",
    "                entity_type = tag[2:]\n",
    "                if entity_type not in entity_counts:\n",
    "                    entity_counts[entity_type] = 0\n",
    "                entity_counts[entity_type] += 1\n",
    "                current_entity_type = entity_type\n",
    "    \n",
    "    \n",
    "    sorted_entities = sorted(entity_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 12))\n",
    "    \n",
    "    # Plot 1: Horizontal bar chart untuk semua entities\n",
    "    entity_names = [item[0] for item in sorted_entities]\n",
    "    entity_values = [item[1] for item in sorted_entities]\n",
    "    \n",
    "    bars = ax1.barh(range(len(entity_names)), entity_values, color=plt.cm.Set3(np.linspace(0, 1, len(entity_names))))\n",
    "    ax1.set_yticks(range(len(entity_names)))\n",
    "    ax1.set_yticklabels(entity_names, fontsize=10)\n",
    "    ax1.set_xlabel('Jumlah Entities')\n",
    "    ax1.set_title('Distribusi Entity Types dalam Dataset', fontsize=14, fontweight='bold')\n",
    "    ax1.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "\n",
    "    for i, bar in enumerate(bars):\n",
    "        width = bar.get_width()\n",
    "        ax1.text(width + max(entity_values)*0.01, bar.get_y() + bar.get_height()/2, \n",
    "                f'{int(width)}', ha='left', va='center', fontsize=9)\n",
    "    \n",
    "    # Plot 2: Pie chart untuk top 10 entities\n",
    "    top_10 = sorted_entities[:10]\n",
    "    others_count = sum([item[1] for item in sorted_entities[10:]])\n",
    "    \n",
    "    pie_labels = [item[0] for item in top_10]\n",
    "    pie_values = [item[1] for item in top_10]\n",
    "    \n",
    "    if others_count > 0:\n",
    "        pie_labels.append('Others')\n",
    "        pie_values.append(others_count)\n",
    "    \n",
    "    wedges, texts, autotexts = ax2.pie(pie_values, labels=pie_labels, autopct='%1.1f%%', \n",
    "                                       startangle=90, colors=plt.cm.Set3(np.linspace(0, 1, len(pie_values))))\n",
    "    ax2.set_title('Distribusi Top 10 Entity Types', fontsize=14, fontweight='bold')\n",
    "\n",
    "    for text in texts:\n",
    "        text.set_fontsize(10)\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_fontsize(9)\n",
    "        autotext.set_color('white')\n",
    "        autotext.set_weight('bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return entity_counts\n",
    "\n",
    "\n",
    "entity_counts = visualize_entity_distribution(df)\n",
    "print(f\"\\nTotal unique entity types: {len(entity_counts)}\")\n",
    "print(f\"Total entities found: {sum(entity_counts.values())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02968145",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aee1df9",
   "metadata": {},
   "source": [
    "# Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e759f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Loading dataset splits from PUBLIC folder...\")\n",
    "\n",
    "train_ids = pd.read_csv('../../Datasets/PUBLIC/train.ids.csv', header=None, names=['id'])\n",
    "print(f\"Train IDs loaded: {len(train_ids)} documents\")\n",
    "\n",
    "\n",
    "val_ids = pd.read_csv('../../Datasets/PUBLIC/val.ids.csv', header=None, names=['id'])\n",
    "print(f\"Validation IDs loaded: {len(val_ids)} documents\")\n",
    "\n",
    "\n",
    "test_ids = pd.read_csv('../../Datasets/PUBLIC/test.ids.csv', header=None, names=['id'])\n",
    "print(f\"Test IDs loaded: {len(test_ids)} documents\")\n",
    "\n",
    "train_ids_set = set(train_ids['id'].tolist())\n",
    "val_ids_set = set(val_ids['id'].tolist())\n",
    "test_ids_set = set(test_ids['id'].tolist())\n",
    "\n",
    "print(f\"\\nDataset split summary:\")\n",
    "print(f\"Train: {len(train_ids_set)} documents\")\n",
    "print(f\"Validation: {len(val_ids_set)} documents\")\n",
    "print(f\"Test: {len(test_ids_set)} documents\")\n",
    "print(f\"Total: {len(train_ids_set) + len(val_ids_set) + len(test_ids_set)} documents\")\n",
    "\n",
    "# Verifikasi tidak ada overlap\n",
    "overlap_train_val = train_ids_set.intersection(val_ids_set)\n",
    "overlap_train_test = train_ids_set.intersection(test_ids_set)\n",
    "overlap_val_test = val_ids_set.intersection(test_ids_set)\n",
    "\n",
    "print(f\"\\nOverlap check:\")\n",
    "print(f\"Train-Val overlap: {len(overlap_train_val)} documents\")\n",
    "print(f\"Train-Test overlap: {len(overlap_train_test)} documents\")\n",
    "print(f\"Val-Test overlap: {len(overlap_val_test)} documents\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d04a6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Splitting dataframe based on predefined IDs...\")\n",
    "\n",
    "# Filter dataframe berdasarkan ID\n",
    "train_df = df[df['id'].isin(train_ids_set)].copy().reset_index(drop=True)\n",
    "val_df = df[df['id'].isin(val_ids_set)].copy().reset_index(drop=True)\n",
    "test_df = df[df['id'].isin(test_ids_set)].copy().reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nDataFrame splits created:\")\n",
    "print(f\"Train DataFrame: {len(train_df)} documents\")\n",
    "print(f\"Validation DataFrame: {len(val_df)} documents\")\n",
    "print(f\"Test DataFrame: {len(test_df)} documents\")\n",
    "\n",
    "\n",
    "total_split = len(train_df) + len(val_df) + len(test_df)\n",
    "print(f\"\\nOriginal dataset: {len(df)} documents\")\n",
    "print(f\"Split total: {total_split} documents\")\n",
    "\n",
    "if total_split == len(df):\n",
    "    print(\"All documents successfully assigned to splits!\")\n",
    "else:\n",
    "    missing = len(df) - total_split\n",
    "    print(f\"{missing} documents are missing from splits\")\n",
    "    \n",
    "   \n",
    "    all_split_ids = train_ids_set.union(val_ids_set).union(test_ids_set)\n",
    "    original_ids = set(df['id'].tolist())\n",
    "    missing_ids = original_ids - all_split_ids\n",
    "    if missing_ids:\n",
    "        print(f\"Missing IDs: {list(missing_ids)[:10]}...\")  # Show first 10\n",
    "\n",
    "\n",
    "print(f\"\\nVerdict distribution per split:\")\n",
    "print(f\"\\nTrain verdict distribution:\")\n",
    "print(train_df['verdict'].value_counts(normalize=True).round(3))\n",
    "\n",
    "print(f\"\\nValidation verdict distribution:\")\n",
    "print(val_df['verdict'].value_counts(normalize=True).round(3))\n",
    "\n",
    "print(f\"\\nTest verdict distribution:\")\n",
    "print(test_df['verdict'].value_counts(normalize=True).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d5f0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Setup model dan tokenizer - using XLM-RoBERTa Large for better multilingual performance\n",
    "print(\"Setting up XLM-RoBERTa Large model untuk Indonesian Legal NER...\")\n",
    "\n",
    "# Load tokenizer untuk XLM-RoBERTa Large model  \n",
    "model_name = \"xlm-roberta-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Loaded XLM-RoBERTa Large tokenizer: {model_name}\")\n",
    "print(f\"Vocab size: {tokenizer.vocab_size}\")\n",
    "print(f\"Special tokens: {tokenizer.special_tokens_map}\")\n",
    "print(f\"CLS token: '{tokenizer.cls_token}' (ID: {tokenizer.cls_token_id})\")\n",
    "print(f\"SEP token: '{tokenizer.sep_token}' (ID: {tokenizer.sep_token_id})\")\n",
    "print(f\"PAD token: '{tokenizer.pad_token}' (ID: {tokenizer.pad_token_id})\")\n",
    "print(f\"UNK token: '{tokenizer.unk_token}' (ID: {tokenizer.unk_token_id})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7083d201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized Training Parameters untuk IndoBERT\n",
    "def get_indobert_optimized_training_args(output_dir):\n",
    "    \"\"\"\n",
    "    Training arguments yang dioptimasi khusus untuk IndoBERT\n",
    "    \"\"\"\n",
    "    from transformers import TrainingArguments\n",
    "    \n",
    "    return TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        \n",
    "        # Optimized for IndoBERT convergence\n",
    "        num_train_epochs=4,              # IndoBERT converges faster\n",
    "        per_device_train_batch_size=8,   # Can handle larger batch\n",
    "        per_device_eval_batch_size=16,   # Increase eval batch\n",
    "        \n",
    "        # Learning rate optimization\n",
    "        learning_rate=2e-5,              # Sweet spot untuk IndoBERT\n",
    "        weight_decay=0.01,\n",
    "        warmup_ratio=0.1,                # 10% warmup steps\n",
    "        \n",
    "        # Gradient optimization\n",
    "        gradient_accumulation_steps=1,\n",
    "        max_grad_norm=1.0,\n",
    "        \n",
    "        # Evaluation strategy\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=150,                  # More frequent evaluation\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=150,\n",
    "        save_total_limit=3,\n",
    "        \n",
    "        # Model selection\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        greater_is_better=True,\n",
    "        \n",
    "        # Performance optimization\n",
    "        fp16=True,                       # Mixed precision training\n",
    "        dataloader_pin_memory=True,\n",
    "        remove_unused_columns=False,\n",
    "        \n",
    "        # Logging\n",
    "        logging_steps=50,\n",
    "        report_to=None,\n",
    "    )\n",
    "\n",
    "print(\"✓ IndoBERT optimized training arguments ready\")\n",
    "print(\"  - Batch size: 8 (train), 16 (eval)\")  \n",
    "print(\"  - Learning rate: 2e-5\")\n",
    "print(\"  - Epochs: 4\")\n",
    "print(\"  - Mixed precision: Enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532fbd1e",
   "metadata": {},
   "source": [
    "Token CLS (classification token) biasanya ditempatkan di awal urutan dan digunakan untuk tugas klasifikasi. Token SEP (separator token) memisahkan segmen input teks yang berbeda. Token PAD digunakan untuk membuat urutan dengan panjang berbeda menjadi seragam untuk pemrosesan batch. Terakhir, token UNK (unknown token) mewakili kata-kata yang tidak ada dalam vocabulary model. Setiap token ditampilkan dengan representasi string dan ID numerik yang sesuai, yang merupakan cara model sebenarnya memproses penanda khusus ini secara internal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11000830",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample_tokens = df.iloc[0]['text'][:100]\n",
    "print(f\"\\n Testing tokenization on sample:\")\n",
    "print(f\"Original tokens: {sample_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d645181",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Tokenize dengan berbagai parameter\n",
    "test_encoding = tokenizer(\n",
    "    sample_tokens,\n",
    "    is_split_into_words=True,\n",
    "    add_special_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    "    return_attention_mask=True\n",
    ")\n",
    "\n",
    "print(f\"\\nTokenization result:\")\n",
    "print(f\"Input IDs: {test_encoding['input_ids']}\")\n",
    "print(f\"Tokens: {tokenizer.convert_ids_to_tokens(test_encoding['input_ids'])}\")\n",
    "print(f\"Word IDs: {test_encoding.word_ids()}\")\n",
    "print(f\"Attention mask: {test_encoding['attention_mask']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52f98bf",
   "metadata": {},
   "source": [
    "```\n",
    "Contoh transformasi:\n",
    "Text: \"John works\"\n",
    "Tokens: [\"[CLS]\", \"John\", \"works\", \"[SEP]\"]\n",
    "input_ids: [101, 2198, 2152, 102]  # Vocab ID untuk setiap token\n",
    "labels: [0, 1, 0, 0]               # 0=O, 1=B-PER untuk \"John\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5a52d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"mapping NER...\")\n",
    "\n",
    "def create_label_mapping(df):\n",
    "\n",
    "    all_labels = set()\n",
    "    \n",
    "    for tags in df['text-tags']:\n",
    "        all_labels.update(tags)\n",
    "    \n",
    "    sorted_labels = sorted(list(all_labels))\n",
    "    \n",
    "\n",
    "    if 'O' in sorted_labels:\n",
    "        sorted_labels.remove('O')\n",
    "        sorted_labels = ['O'] + sorted_labels\n",
    "    \n",
    "    # Buat mapping\n",
    "    label2id = {label: idx for idx, label in enumerate(sorted_labels)}\n",
    "    id2label = {idx: label for label, idx in label2id.items()}\n",
    "    \n",
    "    return label2id, id2label, sorted_labels\n",
    "\n",
    "# Create mappings\n",
    "label2id, id2label, all_labels = create_label_mapping(df)\n",
    "\n",
    "print(f\"\\nLabel mapping created:\")\n",
    "print(f\"Total unique labels: {len(all_labels)}\")\n",
    "print(f\"Label 'O' has ID: {label2id['O']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3606fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0fbab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nSample label mapping (first 15):\")\n",
    "for i, (label, idx) in enumerate(list(label2id.items())[:15]):\n",
    "    print(f\"{idx:2d}: {label}\")\n",
    "\n",
    "if len(label2id) > 15:\n",
    "    print(f\"... dan {len(label2id)-15} labels lainnya\")\n",
    "\n",
    "# Analisis distribusi entity types (tanpa B-/I- prefix)\n",
    "entity_types = set()\n",
    "for label in all_labels:\n",
    "    if label != 'O':\n",
    "        entity_type = label.split('-', 1)[1] if '-' in label else label\n",
    "        entity_types.add(entity_type)\n",
    "\n",
    "print(f\"\\nEntity types found: {len(entity_types)}\")\n",
    "print(f\"Entity types: {sorted(list(entity_types))[:10]}...\")\n",
    "\n",
    "# Hitung distribusi label untuk focal loss\n",
    "label_distribution = Counter()\n",
    "for idx in range(len(df)):\n",
    "    for tag in df.iloc[idx]['text-tags']:\n",
    "        label_distribution[tag] += 1\n",
    "\n",
    "print(f\"\\nLabel distribution for Focal Loss calculation:\")\n",
    "total_labels = sum(label_distribution.values())\n",
    "for label in sorted(label_distribution.keys())[:10]:\n",
    "    count = label_distribution[label]\n",
    "    percentage = (count / total_labels) * 100\n",
    "    print(f\"  {label}: {count:,} ({percentage:.2f}%)\")\n",
    "\n",
    "print(f\"  ... dan {len(label_distribution)-10} labels lainnya\")\n",
    "print(f\"Total labels: {total_labels:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b17913",
   "metadata": {},
   "source": [
    "Tujuan Utama\n",
    "\n",
    "Fungsi ini menyelesaikan masalah kompleks dalam NER: bagaimana menyelaraskan label entitas dengan subword tokens yang dihasilkan oleh tokenizer modern seperti BERT Indonesian.\n",
    "\n",
    "Masalah yang Dipecahkan\n",
    "\n",
    "Ketika Anda memiliki data seperti:\n",
    "\n",
    "Tokens: [\"John\", \"Smith\", \"bekerja\", \"di\", \"Jakarta\"]\n",
    "\n",
    "Labels: [\"B-PER\", \"I-PER\", \"O\", \"O\", \"B-LOC\"]\n",
    "\n",
    "Tokenizer sering memecah kata menjadi subword:\n",
    "\n",
    "\"Jakarta\" → [\"Jak\", \"##arta\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626d0d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementasi Focal Loss untuk mengatasi class imbalance\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from transformers import Trainer\n",
    "import numpy as np\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Focal Loss untuk mengatasi class imbalance dalam NER\n",
    "    Paper: https://arxiv.org/abs/1708.02002\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction='mean', ignore_index=-100):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha  # Will be converted to buffer in forward if tensor\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.ignore_index = ignore_index\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        # Flatten inputs dan targets\n",
    "        inputs = inputs.view(-1, inputs.size(-1))\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        # Filter out ignore_index\n",
    "        mask = targets != self.ignore_index\n",
    "        inputs = inputs[mask]\n",
    "        targets = targets[mask]\n",
    "        \n",
    "        if len(targets) == 0:\n",
    "            return torch.tensor(0.0, requires_grad=True).to(inputs.device)\n",
    "        \n",
    "        # Compute cross entropy\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        \n",
    "        # Compute p_t\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        \n",
    "        # Compute alpha_t jika alpha diberikan\n",
    "        if self.alpha is not None:\n",
    "            if isinstance(self.alpha, (float, int)):\n",
    "                alpha_t = self.alpha\n",
    "            else:\n",
    "                # Ensure alpha is on same device as inputs\n",
    "                if isinstance(self.alpha, torch.Tensor):\n",
    "                    if not hasattr(self, '_alpha_buffer'):\n",
    "                        self.register_buffer('_alpha_buffer', self.alpha.to(inputs.device))\n",
    "                    alpha_t = self._alpha_buffer[targets]\n",
    "                else:\n",
    "                    alpha_t = self.alpha[targets]\n",
    "        else:\n",
    "            alpha_t = 1.0\n",
    "        \n",
    "        # Compute focal loss\n",
    "        focal_loss = alpha_t * (1 - pt) ** self.gamma * ce_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "def calculate_class_weights(label_distribution, label2id, strategy='inverse_freq'):\n",
    "    \"\"\"\n",
    "    Hitung class weights untuk focal loss\n",
    "    \"\"\"\n",
    "    total_samples = sum(label_distribution.values())\n",
    "    num_classes = len(label2id)\n",
    "    \n",
    "    if strategy == 'inverse_freq':\n",
    "        # Inverse frequency weighting\n",
    "        weights = []\n",
    "        for i in range(num_classes):\n",
    "            label = id2label[i]\n",
    "            count = label_distribution.get(label, 1)  # Avoid division by zero\n",
    "            weight = total_samples / (num_classes * count)\n",
    "            weights.append(weight)\n",
    "    \n",
    "    elif strategy == 'balanced':\n",
    "        # Balanced weighting\n",
    "        weights = []\n",
    "        for i in range(num_classes):\n",
    "            label = id2label[i]\n",
    "            count = label_distribution.get(label, 1)\n",
    "            weight = total_samples / (2 * count)  # sklearn-style balanced\n",
    "            weights.append(weight)\n",
    "    \n",
    "    else:  # uniform\n",
    "        weights = [1.0] * num_classes\n",
    "    \n",
    "    # Normalize weights\n",
    "    weights = np.array(weights)\n",
    "    weights = weights / weights.sum() * num_classes\n",
    "    \n",
    "    return torch.FloatTensor(weights)\n",
    "\n",
    "# Hitung class weights dari distribusi label\n",
    "print(\"Calculating class weights for Focal Loss...\")\n",
    "class_weights = calculate_class_weights(label_distribution, label2id, strategy='inverse_freq')\n",
    "\n",
    "print(f\"Class weights calculated for {len(class_weights)} classes\")\n",
    "print(f\"Sample weights (first 10): {class_weights[:10].tolist()}\")\n",
    "print(f\"Weight for 'O' label: {class_weights[label2id['O']]:.4f}\")\n",
    "\n",
    "# Tampilkan entity weights\n",
    "entity_weights = {}\n",
    "for label, weight in zip(id2label.values(), class_weights):\n",
    "    if label != 'O':\n",
    "        entity_type = label.split('-', 1)[1] if '-' in label else label\n",
    "        if entity_type not in entity_weights:\n",
    "            entity_weights[entity_type] = []\n",
    "        entity_weights[entity_type].append(weight.item())\n",
    "\n",
    "print(f\"\\nEntity type weights (average):\")\n",
    "for entity_type in sorted(entity_weights.keys())[:10]:\n",
    "    avg_weight = np.mean(entity_weights[entity_type])\n",
    "    print(f\"  {entity_type}: {avg_weight:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec33dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Trainer dengan Focal Loss\n",
    "class FocalLossTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    Custom Trainer yang menggunakan Focal Loss untuk mengatasi class imbalance\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, focal_loss_alpha=None, focal_loss_gamma=2.0, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # Convert alpha to tensor if it's a list/array\n",
    "        if focal_loss_alpha is not None and not isinstance(focal_loss_alpha, torch.Tensor):\n",
    "            focal_loss_alpha = torch.tensor(focal_loss_alpha, dtype=torch.float32)\n",
    "        self.focal_loss = FocalLoss(\n",
    "            alpha=focal_loss_alpha,\n",
    "            gamma=focal_loss_gamma,\n",
    "            ignore_index=-100\n",
    "        )\n",
    "        \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        \"\"\"\n",
    "        Override compute_loss untuk menggunakan Focal Loss\n",
    "        \"\"\"\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        \n",
    "        # Compute focal loss\n",
    "        loss = self.focal_loss(logits, labels)\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "print(\"Custom FocalLossTrainer defined successfully!\")\n",
    "print(\"Features:\")\n",
    "print(\"  - Focal Loss for class imbalance handling\")\n",
    "print(\"  - Configurable alpha (class weights) and gamma (focusing parameter)\")\n",
    "print(\"  - Automatic handling of ignored tokens (-100)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b6823f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize_and_align_labels(examples_df, tokenizer, label2id, max_length=512):\n",
    "    tokenized_inputs = []\n",
    "    \n",
    "    print(f\"Tokenizing {len(examples_df)} documents...\")\n",
    "    \n",
    "    for idx in range(len(examples_df)):\n",
    "        if idx % 100 == 0: \n",
    "            print(f\"  Processed {idx}/{len(examples_df)} documents...\")\n",
    "            \n",
    "        tokens = examples_df.iloc[idx]['text']\n",
    "        labels = examples_df.iloc[idx]['text-tags']\n",
    "        \n",
    "        \n",
    "        if len(tokens) != len(labels):\n",
    "            print(f\"Length mismatch at index {idx}: tokens={len(tokens)}, labels={len(labels)}\")\n",
    "            continue\n",
    "        \n",
    "        # Tokenisasi dengan preserving word boundaries\n",
    "        try:\n",
    "            tokenized_input = tokenizer(\n",
    "                tokens,\n",
    "                is_split_into_words=True,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=max_length,\n",
    "                return_offsets_mapping=True,\n",
    "                return_attention_mask=True,\n",
    "                return_tensors=None \n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Tokenization error at index {idx}: {e}\")\n",
    "            continue\n",
    "        \n",
    "       \n",
    "        word_ids = tokenized_input.word_ids()\n",
    "        aligned_labels = []\n",
    "        \n",
    "        previous_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                # Special tokens (CLS, SEP, PAD)\n",
    "                aligned_labels.append(-100)  \n",
    "            elif word_idx != previous_word_idx:\n",
    "                \n",
    "                if word_idx < len(labels):  # Safety check\n",
    "                    aligned_labels.append(label2id[labels[word_idx]])\n",
    "                else:\n",
    "                    aligned_labels.append(-100)\n",
    "            else:\n",
    "                # Subsequent subword tokens dari word yang sama\n",
    "                # Set ke -100 (ignore) untuk menghindari duplikasi loss\n",
    "                aligned_labels.append(-100)\n",
    "            \n",
    "            previous_word_idx = word_idx\n",
    "        \n",
    "        # Pastikan panjang aligned_labels sama dengan input_ids\n",
    "        if len(aligned_labels) != len(tokenized_input['input_ids']):\n",
    "            print(f\"Alignment error at index {idx}\")\n",
    "            continue\n",
    "        \n",
    "        tokenized_input['labels'] = aligned_labels\n",
    "        \n",
    "        # Remove offset_mapping karena tidak diperlukan untuk training\n",
    "        del tokenized_input['offset_mapping']\n",
    "        \n",
    "        tokenized_inputs.append(tokenized_input)\n",
    "    \n",
    "    print(f\"Tokenization completed: {len(tokenized_inputs)} documents processed\")\n",
    "    return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce8030d",
   "metadata": {},
   "source": [
    "Logika alignment:\n",
    "\n",
    "Special tokens (CLS, SEP, PAD) → -100 (diabaikan dalam loss calculation)\n",
    "\n",
    "First subword dari setiap kata → mendapat label asli\n",
    "\n",
    "Subsequent subwords → -100 (menghindari duplikasi loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d2680f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Test pada 3 dokumen pertama dari train set\n",
    "sample_df = train_df.head(3)\n",
    "MAX_LENGTH = 512  # Set maximum length\n",
    "\n",
    "tokenized_samples = tokenize_and_align_labels(sample_df, tokenizer, label2id, MAX_LENGTH)\n",
    "\n",
    "print(f\"\\nTokenization test results:\")\n",
    "print(f\"Input samples: {len(sample_df)}\")\n",
    "print(f\"Output samples: {len(tokenized_samples)}\")\n",
    "\n",
    "if tokenized_samples:\n",
    "    # Analisis sample pertama\n",
    "    sample = tokenized_samples[1]\n",
    "    print(f\"\\nSample 0 analysis:\")\n",
    "    print(f\"Input IDs length: {len(sample['input_ids'])}\")\n",
    "    print(f\"Labels length: {len(sample['labels'])}\")\n",
    "    print(f\"Attention mask length: {len(sample['attention_mask'])}\")\n",
    "    print(f\"Max length setting: {MAX_LENGTH}\")\n",
    "    \n",
    "    # Decode beberapa tokens untuk verifikasi\n",
    "    tokens = tokenizer.convert_ids_to_tokens(sample['input_ids'][:50])\n",
    "    labels = sample['labels'][:50]\n",
    "    \n",
    "    print(f\"\\nFirst 50 tokens and labels:\")\n",
    "    print(f\"{'Token':<15} {'Label ID':<8} {'Label Name':<25}\")\n",
    "    print(\"-\" * 50)\n",
    "    for token, label_id in zip(tokens, labels):\n",
    "        label_name = id2label[label_id] if label_id != -100 else \"IGNORE\"\n",
    "        print(f\"{token:<15} {label_id:<8} {label_name:<25}\")\n",
    "    \n",
    "    # Statistik labels\n",
    "    all_label_ids = []\n",
    "    for sample in tokenized_samples:\n",
    "        all_label_ids.extend([l for l in sample['labels'] if l != -100])\n",
    "    \n",
    "    print(f\"\\nLabel statistics in test samples:\")\n",
    "    from collections import Counter\n",
    "    label_counts = Counter(all_label_ids)\n",
    "    for label_id, count in label_counts.most_common(10):\n",
    "        print(f\"{id2label[label_id]:<25}: {count}\")\n",
    "\n",
    "else:\n",
    "    print(\"No samples were successfully tokenized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b582f243",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc2cf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisasi semua dataset splits\n",
    "print(\"Starting full tokenization for all dataset splits...\")\n",
    "print(\"This may take several minutes depending on dataset size...\")\n",
    "\n",
    "# Tentukan max_length berdasarkan analisis panjang dokumen\n",
    "def analyze_document_lengths(df_list, names):\n",
    "    print(\"\\nAnalyzing document lengths...\")\n",
    "    for df, name in zip(df_list, names):\n",
    "        lengths = [len(text) for text in df['text']]\n",
    "        print(f\"\\n{name} set:\")\n",
    "        print(f\"  Mean length: {np.mean(lengths):.1f} tokens\")\n",
    "        print(f\"  Median length: {np.median(lengths):.1f} tokens\")\n",
    "        print(f\"  Max length: {max(lengths)} tokens\")\n",
    "        print(f\"  95th percentile: {np.percentile(lengths, 95):.1f} tokens\")\n",
    "        print(f\"  99th percentile: {np.percentile(lengths, 99):.1f} tokens\")\n",
    "\n",
    "analyze_document_lengths([train_df, val_df, test_df], [\"Train\", \"Validation\", \"Test\"])\n",
    "\n",
    "# Set MAX_LENGTH berdasarkan analisis (bisa disesuaikan)\n",
    "MAX_LENGTH = 512\n",
    "print(f\"\\nUsing MAX_LENGTH = {MAX_LENGTH}\")\n",
    "print(f\"Note: Documents longer than {MAX_LENGTH} will be truncated\")\n",
    "\n",
    "# Tokenisasi train set\n",
    "print(f\"\\nTokenizing training set ({len(train_df)} documents)...\")\n",
    "train_tokenized = tokenize_and_align_labels(train_df, tokenizer, label2id, MAX_LENGTH)\n",
    "\n",
    "# Tokenisasi validation set\n",
    "print(f\"\\nTokenizing validation set ({len(val_df)} documents)...\")\n",
    "val_tokenized = tokenize_and_align_labels(val_df, tokenizer, label2id, MAX_LENGTH)\n",
    "\n",
    "# Tokenisasi test set\n",
    "print(f\"\\nTokenizing test set ({len(test_df)} documents)...\")\n",
    "test_tokenized = tokenize_and_align_labels(test_df, tokenizer, label2id, MAX_LENGTH)\n",
    "\n",
    "print(f\"\\nTokenization completed for all splits!\")\n",
    "print(f\"Final tokenized dataset sizes:\")\n",
    "print(f\"  Train: {len(train_tokenized)} samples\")\n",
    "print(f\"  Validation: {len(val_tokenized)} samples\")\n",
    "print(f\"  Test: {len(test_tokenized)} samples\")\n",
    "print(f\"  Total: {len(train_tokenized) + len(val_tokenized) + len(test_tokenized)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b286d150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc0514e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def analyze_tokenized_data(tokenized_data, dataset_name):\n",
    "\n",
    "    print(f\"\\n{dataset_name} tokenization statistics:\")\n",
    "    \n",
    "    if not tokenized_data:\n",
    "        print(\"  No data to analyze\")\n",
    "        return\n",
    "    \n",
    "    # Analisis panjang sequence\n",
    "    input_lengths = [len(sample['input_ids']) for sample in tokenized_data]\n",
    "    actual_lengths = [sum(sample['attention_mask']) for sample in tokenized_data]  # Non-padded length\n",
    "    \n",
    "    print(f\"    Sequence lengths:\")\n",
    "    print(f\"    Max length (with padding): {max(input_lengths)}\")\n",
    "    print(f\"    Actual length - Mean: {np.mean(actual_lengths):.1f}\")\n",
    "    print(f\"    Actual length - Median: {np.median(actual_lengths):.1f}\")\n",
    "    print(f\"    Actual length - Max: {max(actual_lengths)}\")\n",
    "    print(f\"    Actual length - Min: {min(actual_lengths)}\")\n",
    "    \n",
    "    # Hitung berapa dokumen yang kena truncation\n",
    "    truncated = sum(1 for length in actual_lengths if length == MAX_LENGTH)\n",
    "    print(f\"    Truncated documents: {truncated} ({truncated/len(tokenized_data)*100:.1f}%)\")\n",
    "    \n",
    "    # Analisis distribusi labels\n",
    "    all_labels = []\n",
    "    ignored_tokens = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    for sample in tokenized_data:\n",
    "        for label in sample['labels']:\n",
    "            total_tokens += 1\n",
    "            if label == -100:\n",
    "                ignored_tokens += 1\n",
    "            else:\n",
    "                all_labels.append(label)\n",
    "    \n",
    "    print(f\"    Label statistics:\")\n",
    "    print(f\"    Total tokens: {total_tokens:,}\")\n",
    "    print(f\"    Ignored tokens: {ignored_tokens:,} ({ignored_tokens/total_tokens*100:.1f}%)\")\n",
    "    print(f\"    Valid labels: {len(all_labels):,} ({len(all_labels)/total_tokens*100:.1f}%)\")\n",
    "    \n",
    "    # Distribusi top labels\n",
    "    if all_labels:\n",
    "        label_counts = Counter(all_labels)\n",
    "        print(f\"    Top 10 labels:\")\n",
    "        for label_id, count in label_counts.most_common(10):\n",
    "            label_name = id2label[label_id]\n",
    "            percentage = count / len(all_labels) * 100\n",
    "            print(f\"      {label_name:<25}: {count:>6,} ({percentage:>5.1f}%)\")\n",
    "    \n",
    "    return {\n",
    "        'total_samples': len(tokenized_data),\n",
    "        'avg_length': np.mean(actual_lengths),\n",
    "        'max_length': max(actual_lengths),\n",
    "        'truncated_count': truncated,\n",
    "        'total_tokens': total_tokens,\n",
    "        'valid_labels': len(all_labels),\n",
    "        'ignored_tokens': ignored_tokens\n",
    "    }\n",
    "\n",
    "# Analisis semua splits\n",
    "print(\"Analyzing tokenized datasets...\")\n",
    "\n",
    "train_stats = analyze_tokenized_data(train_tokenized, \"Training\")\n",
    "val_stats = analyze_tokenized_data(val_tokenized, \"Validation\")\n",
    "test_stats = analyze_tokenized_data(test_tokenized, \"Test\")\n",
    "\n",
    "# Summary total\n",
    "print(f\"\\nOverall Summary:\")\n",
    "total_samples = train_stats['total_samples'] + val_stats['total_samples'] + test_stats['total_samples']\n",
    "total_tokens = train_stats['total_tokens'] + val_stats['total_tokens'] + test_stats['total_tokens']\n",
    "total_valid_labels = train_stats['valid_labels'] + val_stats['valid_labels'] + test_stats['valid_labels']\n",
    "\n",
    "print(f\"  Total samples: {total_samples:,}\")\n",
    "print(f\"  Total tokens: {total_tokens:,}\")\n",
    "print(f\"  Total valid labels: {total_valid_labels:,}\")\n",
    "print(f\"  Average tokens per sample: {total_tokens/total_samples:.1f}\")\n",
    "print(f\"  Label density: {total_valid_labels/total_tokens*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbc950b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save hasil preprocessing untuk training\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "\n",
    "print(\"Saving preprocessing results...\")\n",
    "\n",
    "# Buat direktori untuk menyimpan hasil preprocessing\n",
    "output_dir = \"./results\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save tokenized data\n",
    "print(\"  Saving tokenized datasets...\")\n",
    "with open(os.path.join(output_dir, 'train_tokenized.pkl'), 'wb') as f:\n",
    "    pickle.dump(train_tokenized, f)\n",
    "    \n",
    "with open(os.path.join(output_dir, 'val_tokenized.pkl'), 'wb') as f:\n",
    "    pickle.dump(val_tokenized, f)\n",
    "    \n",
    "with open(os.path.join(output_dir, 'test_tokenized.pkl'), 'wb') as f:\n",
    "    pickle.dump(test_tokenized, f)\n",
    "\n",
    "# Save metadata untuk training\n",
    "metadata = {\n",
    "    'model_name': model_name,  # Now 'xlm-roberta-large'\n",
    "    'max_length': MAX_LENGTH,\n",
    "    'num_labels': len(label2id),\n",
    "    'label2id': label2id,\n",
    "    'id2label': {int(k): v for k, v in id2label.items()},  # JSON serializable\n",
    "    'entity_types': sorted(list(entity_types)),\n",
    "    'dataset_splits': {\n",
    "        'train_size': len(train_tokenized),\n",
    "        'val_size': len(val_tokenized),\n",
    "        'test_size': len(test_tokenized)\n",
    "    },\n",
    "    'statistics': {\n",
    "        'train': train_stats,\n",
    "        'val': val_stats,\n",
    "        'test': test_stats\n",
    "    },\n",
    "    'preprocessing_date': pd.Timestamp.now().isoformat(),\n",
    "    'architecture': 'XLM-RoBERTa Large',\n",
    "    'loss_function': 'Focal Loss',\n",
    "    'class_imbalance_handling': 'Enabled'\n",
    "}\n",
    "\n",
    "print(\"  Saving metadata...\")\n",
    "with open(os.path.join(output_dir, 'preprocessing_metadata.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Save original dataframes untuk referensi\n",
    "print(\"  Saving original DataFrames...\")\n",
    "train_df.to_csv(os.path.join(output_dir, 'train_df.csv'), index=False)\n",
    "val_df.to_csv(os.path.join(output_dir, 'val_df.csv'), index=False)\n",
    "test_df.to_csv(os.path.join(output_dir, 'test_df.csv'), index=False)\n",
    "\n",
    "# Save label mappings secara terpisah\n",
    "print(\"  Saving label mappings...\")\n",
    "with open(os.path.join(output_dir, 'label2id.json'), 'w') as f:\n",
    "    json.dump(label2id, f, indent=2)\n",
    "    \n",
    "with open(os.path.join(output_dir, 'id2label.json'), 'w') as f:\n",
    "    json.dump({int(k): v for k, v in id2label.items()}, f, indent=2)\n",
    "\n",
    "print(f\"\\nPreprocessing results saved to '{output_dir}' directory:\")\n",
    "print(f\"  Files saved:\")\n",
    "for filename in os.listdir(output_dir):\n",
    "    file_path = os.path.join(output_dir, filename)\n",
    "    file_size = os.path.getsize(file_path) / (1024*1024)  # MB\n",
    "    print(f\"    {filename} ({file_size:.1f} MB)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820bf7d8",
   "metadata": {},
   "source": [
    "### Output Files in `./results/`:\n",
    "\n",
    "- `train_tokenized.pkl` - Training data\n",
    "- `val_tokenized.pkl` - Validation data\n",
    "- `test_tokenized.pkl` - Test data\n",
    "- `preprocessing_metadata.json` - Complete metadata\n",
    "- `label2id.json` & `id2label.json` - Label mappings\n",
    "- `train_df.csv`, `val_df.csv`, `test_df.csv` - Original splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d832db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce40ffaf",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d256a8",
   "metadata": {},
   "source": [
    "# Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13380ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data and setup Indonesian BERT NER model\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification\n",
    ")\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "from seqeval.metrics import f1_score, precision_score, recall_score, classification_report as seq_classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913dd1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data and setup Indonesian BERT NER model\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification\n",
    ")\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "from seqeval.metrics import f1_score, precision_score, recall_score, classification_report as seq_classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd22af78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"NER Model\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "\n",
    "# Load preprocessed data\n",
    "preprocessed_dir = './results'\n",
    "\n",
    "print(\"\\nLoading preprocessed data...\")\n",
    "\n",
    "# Load tokenized datasets\n",
    "with open(os.path.join(preprocessed_dir, 'train_tokenized.pkl'), 'rb') as f:\n",
    "    train_dataset = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(preprocessed_dir, 'val_tokenized.pkl'), 'rb') as f:\n",
    "    val_dataset = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(preprocessed_dir, 'test_tokenized.pkl'), 'rb') as f:\n",
    "    test_dataset = pickle.load(f)\n",
    "\n",
    "# Load metadata and label mappings\n",
    "with open(os.path.join(preprocessed_dir, 'preprocessing_metadata.json'), 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "    \n",
    "with open(os.path.join(preprocessed_dir, 'label2id.json'), 'r') as f:\n",
    "    label2id = json.load(f)\n",
    "    \n",
    "with open(os.path.join(preprocessed_dir, 'id2label.json'), 'r') as f:\n",
    "    id2label = {int(k): v for k, v in json.load(f).items()}\n",
    "\n",
    "print(f\"Loaded {len(train_dataset)} training samples\")\n",
    "print(f\"Loaded {len(val_dataset)} validation samples\")\n",
    "print(f\"Loaded {len(test_dataset)} test samples\")\n",
    "print(f\"Number of labels: {len(label2id)}\")\n",
    "print(f\"Model name: {metadata['model_name']}\")\n",
    "print(f\"Max length: {metadata['max_length']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd04694",
   "metadata": {},
   "source": [
    "Mengkonversi list Python menjadi torch.tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff1c260",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Custom Dataset class for NER\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, tokenized_data):\n",
    "        self.data = tokenized_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        return {\n",
    "            'input_ids': torch.tensor(item['input_ids'], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(item['attention_mask'], dtype=torch.long),\n",
    "            'labels': torch.tensor(item['labels'], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create dataset objects\n",
    "train_torch_dataset = NERDataset(train_dataset)\n",
    "val_torch_dataset = NERDataset(val_dataset)\n",
    "test_torch_dataset = NERDataset(test_dataset)\n",
    "\n",
    "print(f\"\\nCreated PyTorch datasets\")\n",
    "print(f\"  Training: {len(train_torch_dataset)} samples\")\n",
    "print(f\"  Validation: {len(val_torch_dataset)} samples\")\n",
    "print(f\"  Test: {len(test_torch_dataset)} samples\")\n",
    "\n",
    "# Load model and tokenizer - Updated to use XLM-RoBERTa Large\n",
    "model_name = \"xlm-roberta-large\"  # Changed from IndoBERT to XLM-RoBERTa Large\n",
    "print(f\"\\nLoading XLM-RoBERTa Large model: {model_name} for token classification...\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(f\"Tokenizer loaded - Vocab size: {len(tokenizer)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e96a85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_torch_dataset.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc8858d",
   "metadata": {},
   "source": [
    "num_labels: Jumlah class NER yang akan diprediksi\n",
    "\n",
    "id2label & label2id: Mapping untuk interpretasi output\n",
    "\n",
    "ignore_mismatched_sizes: Mengabaikan perbedaan ukuran classification head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccff45f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load model for token classification\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(label2id),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "print(f\"  Model loaded and moved to {device}\")\n",
    "print(f\"  Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"  Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "print(f\"  Number of labels: {model.num_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8b647d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b707ade0",
   "metadata": {},
   "source": [
    "menggabungkan multiple samples menjadi satu batch untuk training, untuk pararel processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212be850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator for token classification\n",
    "data_collator = DataCollatorForTokenClassification(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0612cb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    # Get predictions (argmax of logits)\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    for prediction, label in zip(predictions, labels):\n",
    "        valid_indices = label != -100\n",
    "\n",
    "        pred_labels = [id2label[p] for p, valid in zip(prediction, valid_indices) if valid]\n",
    "        true_label_list = [id2label[l] for l, valid in zip(label, valid_indices) if valid]\n",
    "\n",
    "        if len(pred_labels) > 0 and len(true_label_list) > 0:\n",
    "            true_predictions.append(pred_labels)\n",
    "            true_labels.append(true_label_list)\n",
    "\n",
    "    # Compute seqeval metrics\n",
    "    precision = precision_score(true_labels, true_predictions)\n",
    "    recall = recall_score(true_labels, true_predictions)\n",
    "    f1 = f1_score(true_labels, true_predictions)\n",
    "\n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10c7e28",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2621f4",
   "metadata": {},
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c1d872",
   "metadata": {},
   "source": [
    "- 3 epochs - Cukup untuk fine-tuning pre-trained model tanpa overfitting\n",
    "- Small batch size (4) - BERT model membutuhkan banyak memory, batch kecil mencegah OOM\n",
    "- Larger eval batch (6) - Inference lebih memory-efficient, bisa gunakan batch lebih besar\n",
    "- 2e-5 - Learning rate optimal untuk fine-tuning transformer (tidak terlalu aggressive)\n",
    "- Weight decay 0.01 - Mencegah overfitting dengan L2 regularization\n",
    "- Warmup 10% - Gradual increase LR untuk stable training start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108a1b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments optimized for XLM-RoBERTa Large with Focal Loss\n",
    "output_dir = './models/xlm_roberta_ner_results'\n",
    "logging_dir = './models/cache/xlm_roberta_ner_logs'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    \n",
    "    # Training hyperparameters - optimized for XLM-RoBERTa Large\n",
    "    num_train_epochs=2,  # Reduced for large model to prevent overfitting\n",
    "    per_device_train_batch_size=2,  # Smaller batch for large model memory requirements\n",
    "    per_device_eval_batch_size=4,   # Slightly larger for evaluation\n",
    "    learning_rate=1e-5,  # Lower learning rate for large pre-trained model\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    \n",
    "    # Logging and evaluation\n",
    "    logging_dir=logging_dir,\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    save_total_limit=3,\n",
    "    \n",
    "    # Other settings\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    report_to=None, \n",
    "    \n",
    "    # Performance optimization for large model\n",
    "    gradient_accumulation_steps=2,  # Effective batch size = 2 * 2 = 4\n",
    "    fp16=True,  # Mixed precision for memory efficiency\n",
    "    dataloader_pin_memory=False,\n",
    "    remove_unused_columns=False,\n",
    "    \n",
    "    # For debugging\n",
    "    # max_steps=100,\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Training arguments configured for XLM-RoBERTa Large:\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Train batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Eval batch size: {training_args.per_device_eval_batch_size}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Mixed precision: {training_args.fp16}\")\n",
    "print(f\"  Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171b98b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9636a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer with Focal Loss\n",
    "trainer = FocalLossTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_torch_dataset,\n",
    "    eval_dataset=val_torch_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    focal_loss_alpha=class_weights,  # Use calculated class weights\n",
    "    focal_loss_gamma=2.0,  # Standard focal loss gamma\n",
    ")\n",
    "\n",
    "print(f\"\\nFocal Loss Training Setup Summary:\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Loss function: Focal Loss (alpha=class_weights, gamma=2.0)\")\n",
    "print(f\"Total labels: {len(label2id)}\")\n",
    "print(f\"Training samples: {len(train_torch_dataset):,}\")\n",
    "print(f\"Validation samples: {len(val_torch_dataset):,}\")\n",
    "print(f\"Test samples: {len(test_torch_dataset):,}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"Class imbalance handling: ✓ Enabled with Focal Loss\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aff842a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up memory\n",
    "import gc\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"\\nGPU Memory after cleanup: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78101458",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7b0ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting Model Training.\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Train the model\n",
    "    train_result = trainer.train()\n",
    "    \n",
    "    # Training completed\n",
    "    end_time = time.time()\n",
    "    training_duration = end_time - start_time\n",
    "    \n",
    "    print(f\"\\nTraining completed successfully!\")\n",
    "    print(f\"Training duration: {training_duration/60:.2f} minutes\")\n",
    "    print(f\"Final training loss: {train_result.training_loss:.4f}\")\n",
    "    \n",
    "    # Save the model\n",
    "    trainer.save_model()\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "    print(f\"\\nModel and tokenizer saved to: {output_dir}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nTraining failed with error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9ae30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nEvaluating on validation set...\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "try:\n",
    "    eval_results = trainer.evaluate()\n",
    "    \n",
    "    print(f\"\\n✓ Validation Results:\")\n",
    "    print(f\"  Validation Loss: {eval_results['eval_loss']:.4f}\")\n",
    "    print(f\"  Precision: {eval_results['eval_precision']:.4f}\")\n",
    "    print(f\"  Recall: {eval_results['eval_recall']:.4f}\")\n",
    "    print(f\"  F1-Score: {eval_results['eval_f1']:.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nValidation failed with error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f80658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed evaluation on test set\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "try:\n",
    "    # Get predictions on test set\n",
    "    test_predictions = trainer.predict(test_torch_dataset)\n",
    "    \n",
    "    # Process predictions\n",
    "    predictions = np.argmax(test_predictions.predictions, axis=2)\n",
    "    labels = test_predictions.label_ids\n",
    "    \n",
    "    # Convert to label names (excluding special tokens)\n",
    "    true_predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    for prediction, label in zip(predictions, labels):\n",
    "        valid_indices = label != -100\n",
    "        \n",
    "        pred_labels = [id2label[p] for p, valid in zip(prediction, valid_indices) if valid]\n",
    "        true_label_list = [id2label[l] for l, valid in zip(label, valid_indices) if valid]\n",
    "        \n",
    "        if len(pred_labels) > 0 and len(true_label_list) > 0:\n",
    "            true_predictions.append(pred_labels)\n",
    "            true_labels.append(true_label_list)\n",
    "    \n",
    "    # Compute detailed metrics\n",
    "    precision = precision_score(true_labels, true_predictions)\n",
    "    recall = recall_score(true_labels, true_predictions)\n",
    "    f1 = f1_score(true_labels, true_predictions)\n",
    "    \n",
    "    print(f\"\\n✓ Test Set Results:\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}\")\n",
    "    print(f\"  F1-Score: {f1:.4f}\")\n",
    "    \n",
    "    # Detailed classification report\n",
    "    print(f\"\\nDetailed Classification Report:\")\n",
    "    print(\"=\" * 60)\n",
    "    report = seq_classification_report(true_labels, true_predictions, digits=4)\n",
    "    print(report)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nTest evaluation failed with error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaec02c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entity-wise performance analysis\n",
    "print(\"\\nEntity-wise Performance Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    # Flatten all predictions and labels\n",
    "    all_true_labels = []\n",
    "    all_pred_labels = []\n",
    "    \n",
    "    for true_seq, pred_seq in zip(true_labels, true_predictions):\n",
    "        all_true_labels.extend(true_seq)\n",
    "        all_pred_labels.extend(pred_seq)\n",
    "    \n",
    "    # Get unique entity types (non-O labels)\n",
    "    entity_types = set()\n",
    "    for label in all_true_labels + all_pred_labels:\n",
    "        if label != 'O' and not label.startswith('I-'):\n",
    "            if label.startswith('B-'):\n",
    "                entity_types.add(label[2:])\n",
    "            else:\n",
    "                entity_types.add(label)\n",
    "    \n",
    "    entity_types = sorted(entity_types)\n",
    "    \n",
    "    print(f\"Found {len(entity_types)} entity types for analysis:\")\n",
    "    for i, entity_type in enumerate(entity_types, 1):\n",
    "        print(f\"  {i:2d}. {entity_type}\")\n",
    "    \n",
    "    # Count entities per type\n",
    "    true_entity_counts = Counter()\n",
    "    pred_entity_counts = Counter()\n",
    "    \n",
    "    # Extract entities from sequences\n",
    "    def extract_entities(labels):\n",
    "        entities = []\n",
    "        current_entity = None\n",
    "        current_type = None\n",
    "        start_idx = None\n",
    "        \n",
    "        for idx, label in enumerate(labels):\n",
    "            if label.startswith('B-'):\n",
    "                # Save previous entity if exists\n",
    "                if current_entity is not None:\n",
    "                    entities.append((start_idx, idx-1, current_type))\n",
    "                # Start new entity\n",
    "                current_type = label[2:]\n",
    "                current_entity = label\n",
    "                start_idx = idx\n",
    "            elif label.startswith('I-') and current_entity is not None:\n",
    "                # Continue current entity\n",
    "                current_entity += ' ' + label\n",
    "            else:  # O tag\n",
    "                # End current entity if exists\n",
    "                if current_entity is not None:\n",
    "                    entities.append((start_idx, idx-1, current_type))\n",
    "                    current_entity = None\n",
    "                    current_type = None\n",
    "                    start_idx = None\n",
    "        \n",
    "        # Handle entity at end of sequence\n",
    "        if current_entity is not None:\n",
    "            entities.append((start_idx, len(labels)-1, current_type))\n",
    "        \n",
    "        return entities\n",
    "    \n",
    "    # Extract entities from all sequences\n",
    "    for true_seq, pred_seq in zip(true_labels, true_predictions):\n",
    "        true_entities = extract_entities(true_seq)\n",
    "        pred_entities = extract_entities(pred_seq)\n",
    "        \n",
    "        for _, _, entity_type in true_entities:\n",
    "            true_entity_counts[entity_type] += 1\n",
    "        \n",
    "        for _, _, entity_type in pred_entities:\n",
    "            pred_entity_counts[entity_type] += 1\n",
    "    \n",
    "    # Display entity counts\n",
    "    print(f\"\\nEntity Counts Comparison:\")\n",
    "    print(f\"{'Entity Type':<20} {'True':<8} {'Predicted':<10} {'Difference':<10}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for entity_type in entity_types:\n",
    "        true_count = true_entity_counts.get(entity_type, 0)\n",
    "        pred_count = pred_entity_counts.get(entity_type, 0)\n",
    "        diff = pred_count - true_count\n",
    "        \n",
    "        print(f\"{entity_type:<20} {true_count:<8} {pred_count:<10} {diff:+<10}\")\n",
    "    \n",
    "    total_true = sum(true_entity_counts.values())\n",
    "    total_pred = sum(pred_entity_counts.values())\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'TOTAL':<20} {total_true:<8} {total_pred:<10} {total_pred-total_true:+<10}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nEntity analysis failed with error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf42eaa3",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc91ce4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference example\n",
    "print(\"\\nTesting Model Inference\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "try:\n",
    "    # Example Indonesian legal text for testing\n",
    "    test_text = \"Majelis Hakim yang diketuai oleh Budi Santoso telah memutuskan bahwa terdakwa Ahmad Rahman terbukti melakukan tindak pidana korupsi dengan putusan nomor 123/Pid.Sus-TPK/2023/PN.Jkt.Pst.\"\n",
    "    \n",
    "    print(f\"Test text: {test_text}\")\n",
    "    print(f\"\\nTokenizing and predicting...\")\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        test_text,  \n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Move to device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Predict\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.argmax(outputs.logits, dim=2)\n",
    "    \n",
    "    # Decode predictions\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "    predicted_labels = [id2label[pred.item()] for pred in predictions[0]]\n",
    "    \n",
    "    print(f\"\\nToken-Label Predictions:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for token, label in zip(tokens, predicted_labels):\n",
    "        if token not in ['<s>', '</s>', '<pad>']:\n",
    "            # Clean up token display\n",
    "            clean_token = token.replace('▁', ' ').strip()\n",
    "            if clean_token:\n",
    "                print(f\"{clean_token:<20} -> {label}\")\n",
    "    \n",
    "    # Extract entities\n",
    "    entities = []\n",
    "    current_entity = []\n",
    "    current_label = None\n",
    "    \n",
    "    for token, label in zip(tokens, predicted_labels):\n",
    "        if token in ['<s>', '</s>', '<pad>']:\n",
    "            continue\n",
    "            \n",
    "        clean_token = token.replace('▁', ' ').strip()\n",
    "        if not clean_token:\n",
    "            continue\n",
    "            \n",
    "        if label.startswith('B-'):\n",
    "            # Save previous entity\n",
    "            if current_entity:\n",
    "                entities.append((' '.join(current_entity).strip(), current_label[2:]))\n",
    "            # Start new entity\n",
    "            current_entity = [clean_token]\n",
    "            current_label = label\n",
    "        elif label.startswith('I-') and current_label and label[2:] == current_label[2:]:\n",
    "            # Continue entity\n",
    "            current_entity.append(clean_token)\n",
    "        else:\n",
    "            # End current entity\n",
    "            if current_entity:\n",
    "                entities.append((' '.join(current_entity).strip(), current_label[2:]))\n",
    "            current_entity = []\n",
    "            current_label = None\n",
    "    \n",
    "    # Add final entity if exists\n",
    "    if current_entity:\n",
    "        entities.append((' '.join(current_entity).strip(), current_label[2:]))\n",
    "    \n",
    "    print(f\"\\nExtracted Entities:\")\n",
    "    print(\"-\" * 40)\n",
    "    if entities:\n",
    "        for entity_text, entity_type in entities:\n",
    "            print(f\"{entity_type:<15}: {entity_text}\")\n",
    "    else:\n",
    "        print(\"No entities detected\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\nInference failed with error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8517a1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"XLM-RoBERTa Large Legal NER Model - TRAINING COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n✨ FINAL RESULTS SUMMARY:\")\n",
    "print(f\"  Model: XLM-RoBERTa Large (xlm-roberta-large)\")\n",
    "print(f\"  Loss Function: Focal Loss (α=class_weights, γ=2.0)\")\n",
    "print(f\"  Task: Named Entity Recognition (NER)\")\n",
    "print(f\"  Domain: Indonesian Legal Documents\")\n",
    "print(f\"  Entity Types: {len([l for l in label2id.keys() if l != 'O'])} types\")\n",
    "print(f\"  Training Samples: {len(train_torch_dataset):,}\")\n",
    "print(f\"  Validation Samples: {len(val_torch_dataset):,}\")\n",
    "print(f\"  Test Samples: {len(test_torch_dataset):,}\")\n",
    "print(f\"  Class Imbalance Handling: ✅ Focal Loss Applied\")\n",
    "\n",
    "if 'eval_results' in locals():\n",
    "    print(f\"\\n📊 VALIDATION PERFORMANCE:\")\n",
    "    print(f\"  F1-Score: {eval_results['eval_f1']:.4f}\")\n",
    "    print(f\"  Precision: {eval_results['eval_precision']:.4f}\")\n",
    "    print(f\"  Recall: {eval_results['eval_recall']:.4f}\")\n",
    "\n",
    "if 'f1' in locals():\n",
    "    print(f\"\\n🎯 TEST PERFORMANCE:\")\n",
    "    print(f\"  F1-Score: {f1:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}\")\n",
    "\n",
    "print(f\"\\n📁 MODEL ARTIFACTS:\")\n",
    "print(f\"  Trained model: {output_dir}/\")\n",
    "print(f\"  Preprocessed data: ./preprocessed_data/\")\n",
    "print(f\"  Training logs: {logging_dir}/\")\n",
    "\n",
    "print(f\"\\n🚀 DEPLOYMENT READY:\")\n",
    "print(f\"  Large-scale multilingual model with enhanced Indonesian legal understanding\")\n",
    "print(f\"  Focal Loss implementation for superior minority class detection\")\n",
    "print(f\"  You can load it with: AutoModelForTokenClassification.from_pretrained('{output_dir}')\")\n",
    "print(f\"\\n🎉 XLM-RoBERTa Large + Focal Loss NER Training Successfully Completed!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13ddb05",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a826bd",
   "metadata": {},
   "source": [
    "# Deployment Preparation\n",
    "\n",
    "Preparing model and preprocessing components for deployment to Hugging Face\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b58a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models/model_final directory and save all deployment artifacts\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "model_final_dir = \"./models/model_final\"\n",
    "os.makedirs(model_final_dir, exist_ok=True)\n",
    "\n",
    "print(\"Preparing model for deployment...\")\n",
    "print(f\"Saving to: {model_final_dir}\")\n",
    "\n",
    "# 1. Save the trained model and tokenizer\n",
    "print(\"\\n1. Saving trained model and tokenizer...\")\n",
    "model_artifacts_dir = os.path.join(model_final_dir, \"model_artifacts\")\n",
    "os.makedirs(model_artifacts_dir, exist_ok=True)\n",
    "\n",
    "# Copy trained model files\n",
    "if os.path.exists(output_dir):\n",
    "    for file in os.listdir(output_dir):\n",
    "        src = os.path.join(output_dir, file)\n",
    "        dst = os.path.join(model_artifacts_dir, file)\n",
    "        if os.path.isfile(src):\n",
    "            shutil.copy2(src, dst)\n",
    "    print(f\"Model files copied to {model_artifacts_dir}\")\n",
    "else:\n",
    "    print(f\"Model directory not found: {output_dir}\")\n",
    "\n",
    "# 2. Save label mappings and metadata\n",
    "print(\"\\n2. Saving label mappings and metadata...\")\n",
    "\n",
    "# Enhanced label mappings\n",
    "label_mappings = {\n",
    "    \"label2id\": label2id,\n",
    "    \"id2label\": {int(k): v for k, v in id2label.items()},\n",
    "    \"num_labels\": len(label2id),\n",
    "    \"entity_types\": sorted([label[2:] for label in label2id.keys() if label.startswith('B-')]),\n",
    "    \"all_labels\": list(label2id.keys())\n",
    "}\n",
    "\n",
    "with open(os.path.join(model_final_dir, \"label_mappings.json\"), 'w', encoding='utf-8') as f:\n",
    "    json.dump(label_mappings, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Model configuration for deployment\n",
    "model_config = {\n",
    "    \"model_name\": \"xlm-roberta-large\",\n",
    "    \"task\": \"token-classification\",\n",
    "    \"domain\": \"Indonesian Legal Documents\",\n",
    "    \"language\": \"Indonesian\",\n",
    "    \"max_length\": metadata['max_length'],\n",
    "    \"num_labels\": len(label2id),\n",
    "    \"model_size\": \"large\",\n",
    "    \"architecture\": \"XLM-RoBERTa Large\",\n",
    "    \"loss_function\": \"Focal Loss\",\n",
    "    \"class_imbalance_handling\": \"enabled\",\n",
    "    \"framework\": \"transformers\",\n",
    "    \"entity_types\": label_mappings[\"entity_types\"],\n",
    "    \"training_date\": datetime.now().isoformat(),\n",
    "    \"model_version\": \"2.0.0\",  # Updated version for XLM-RoBERTa + Focal Loss\n",
    "    \"improvements\": [\n",
    "        \"Upgraded to XLM-RoBERTa Large (560M parameters)\",\n",
    "        \"Implemented Focal Loss for class imbalance\",\n",
    "        \"Enhanced multilingual capabilities\",\n",
    "        \"Better minority entity detection\"\n",
    "    ],\n",
    "    \"performance\": {\n",
    "        \"validation_f1\": eval_results.get('eval_f1', 0) if 'eval_results' in locals() else 0,\n",
    "        \"test_f1\": f1 if 'f1' in locals() else 0\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(os.path.join(model_final_dir, \"model_config.json\"), 'w', encoding='utf-8') as f:\n",
    "    json.dump(model_config, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Label mappings saved\")\n",
    "print(f\"Model configuration saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
