{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0c9be35",
   "metadata": {},
   "source": [
    "# Library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8682acae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -r ../../requirements.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb13ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2921c942",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "import torch\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010e9a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dependencies check:\")\n",
    "\n",
    "try:\n",
    "    import transformers\n",
    "    print(f\"transformers: {transformers.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"transformers not installed\")\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"torch: {torch.__version__}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "except ImportError:\n",
    "    print(\"torch not installed\")\n",
    "\n",
    "try:\n",
    "    import datasets\n",
    "    print(f\"datasets: {datasets.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"datasets not installed\")\n",
    "\n",
    "try:\n",
    "    import evaluate\n",
    "    print(f\"evaluate: {evaluate.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"evaluate not installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa957a24",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88aab89c",
   "metadata": {},
   "source": [
    "# EDA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d9d783",
   "metadata": {},
   "source": [
    "- https://github.com/ir-nlp-csui/indoler\n",
    "- https://putusan3.mahkamahagung.go.id/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7f0611",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with open('../../Datasets/PUBLIC/data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "df = pd.json_normalize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88903f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1ae5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"Shape DataFrame: {df.shape}\")\n",
    "print(f\"\\nColumns DataFrame:\")\n",
    "print(df.columns.tolist())\n",
    "print(f\"\\nInfo DataFrame:\")\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcb88df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_all_tags(df):\n",
    "    all_tags = []\n",
    "    for tags in df['text-tags']:\n",
    "        all_tags.extend(tags)\n",
    "    return Counter(all_tags)\n",
    "\n",
    "# distribusi tags\n",
    "tag_counts = get_all_tags(df)\n",
    "print(\"Distribusi Tags NER:\")\n",
    "for tag, count in tag_counts.most_common():\n",
    "    print(f\"{tag}: {count:,}\")\n",
    "\n",
    "#persentase non-O tags\n",
    "total_tags = sum(tag_counts.values())\n",
    "non_o_tags = sum(count for tag, count in tag_counts.items() if tag != 'O')\n",
    "print(f\"\\nTotal tags: {total_tags:,}\")\n",
    "print(f\"Non-O tags: {non_o_tags:,} ({non_o_tags/total_tags*100:.2f}%)\")\n",
    "print(f\"O tags: {tag_counts['O']:,} ({tag_counts['O']/total_tags*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bc6eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def analyze_entity_types(df):\n",
    "\n",
    "    entity_types = set()\n",
    "    entity_examples = {}\n",
    "    \n",
    "    for idx in range(len(df)):\n",
    "        text = df.iloc[idx]['text']\n",
    "        tags = df.iloc[idx]['text-tags']\n",
    "        \n",
    "        current_entity = []\n",
    "        current_entity_type = None\n",
    "        \n",
    "        for token, tag in zip(text, tags):\n",
    "            if tag.startswith('B-'):\n",
    "                # Simpan entity sebelumnya\n",
    "                if current_entity and current_entity_type:\n",
    "                    entity_text = ' '.join(current_entity)\n",
    "                    if current_entity_type not in entity_examples:\n",
    "                        entity_examples[current_entity_type] = []\n",
    "                    if len(entity_examples[current_entity_type]) < 5: \n",
    "                        entity_examples[current_entity_type].append(entity_text)\n",
    "                \n",
    "                # Mulai entity baru\n",
    "                current_entity_type = tag[2:]\n",
    "                entity_types.add(current_entity_type)\n",
    "                current_entity = [token]\n",
    "            \n",
    "            elif tag.startswith('I-') and current_entity_type == tag[2:]:\n",
    "                current_entity.append(token)\n",
    "            \n",
    "            else:  # tag == 'O' atau inconsistent I-tag\n",
    "        \n",
    "                if current_entity and current_entity_type:\n",
    "                    entity_text = ' '.join(current_entity)\n",
    "                    if current_entity_type not in entity_examples:\n",
    "                        entity_examples[current_entity_type] = []\n",
    "                    if len(entity_examples[current_entity_type]) < 5:\n",
    "                        entity_examples[current_entity_type].append(entity_text)\n",
    "                \n",
    "                current_entity = []\n",
    "                current_entity_type = None\n",
    "        \n",
    "        \n",
    "        if current_entity and current_entity_type:\n",
    "            entity_text = ' '.join(current_entity)\n",
    "            if current_entity_type not in entity_examples:\n",
    "                entity_examples[current_entity_type] = []\n",
    "            if len(entity_examples[current_entity_type]) < 5:\n",
    "                entity_examples[current_entity_type].append(entity_text)\n",
    "    \n",
    "    return entity_types, entity_examples\n",
    "\n",
    "\n",
    "print(\"Menganalisis jenis-jenis entity...\")\n",
    "entity_types, entity_examples = analyze_entity_types(df)\n",
    "\n",
    "print(f\"\\nDitemukan {len(entity_types)} jenis entity:\")\n",
    "for entity_type in sorted(entity_types):\n",
    "    print(f\"\\n• {entity_type}:\")\n",
    "    if entity_type in entity_examples:\n",
    "        for example in entity_examples[entity_type]:\n",
    "            print(f\"  - '{example}'\")\n",
    "    else:\n",
    "        print(f\"  - (tidak ada contoh)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381fe0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_annotation_consistency(df):\n",
    "    errors = []\n",
    "    \n",
    "    for idx in range(len(df)):\n",
    "        tags = df.iloc[idx]['text-tags']\n",
    "        text = df.iloc[idx]['text']\n",
    "        \n",
    "        for i, tag in enumerate(tags):\n",
    "            # Check 1: I-tag harus didahului oleh B-tag atau I-tag dengan tipe yang sama\n",
    "            if tag.startswith('I-'):\n",
    "                entity_type = tag[2:]\n",
    "                if i == 0:  # I-tag di awal sequence\n",
    "                    errors.append({\n",
    "                        'doc_id': idx,\n",
    "                        'position': i,\n",
    "                        'error': f'I-tag at beginning: {tag}',\n",
    "                        'context': ' '.join(text[max(0, i-2):i+3])\n",
    "                    })\n",
    "                else:\n",
    "                    prev_tag = tags[i-1]\n",
    "                    if not (prev_tag == f'B-{entity_type}' or prev_tag == f'I-{entity_type}'):\n",
    "                        errors.append({\n",
    "                            'doc_id': idx,\n",
    "                            'position': i,\n",
    "                            'error': f'I-tag without proper B-tag: {prev_tag} -> {tag}',\n",
    "                            'context': ' '.join(text[max(0, i-2):i+3])\n",
    "                        })\n",
    "    \n",
    "    return errors\n",
    "\n",
    "\n",
    "print(\"Mengecek konsistensi anotasi BIO...\")\n",
    "errors = check_annotation_consistency(df)\n",
    "\n",
    "if errors:\n",
    "    print(f\"\\nDitemukan {len(errors)} error konsistensi:\")\n",
    "    for i, error in enumerate(errors[:10]):\n",
    "        print(f\"{i+1}. Doc {error['doc_id']}, Pos {error['position']}: {error['error']}\")\n",
    "        print(f\"   Context: '{error['context']}'\")\n",
    "    \n",
    "    if len(errors) > 10:\n",
    "        print(f\"\\n... dan {len(errors)-10} error lainnya\")\n",
    "else:\n",
    "    print(\"\\nTidak ditemukan error konsistensi BIO!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23edeebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def display_annotations(df, idx, max_tokens=50):\n",
    "    \"\"\"\n",
    "    Menampilkan teks dengan highlight anotasi untuk review manual\n",
    "    \"\"\"\n",
    "    text = df.iloc[idx]['text']\n",
    "    tags = df.iloc[idx]['text-tags']\n",
    "    \n",
    "    print(f\"\\n=== DOCUMENT {idx} (ID: {df.iloc[idx]['id']}) ===\")\n",
    "    print(f\"Verdict: {df.iloc[idx]['verdict']}\")\n",
    "    print(f\"Indictment: {df.iloc[idx]['indictment']}\")\n",
    "    print(f\"Lawyer: {df.iloc[idx]['lawyer']}\")\n",
    "    print(f\"Owner: {df.iloc[idx]['owner']}\")\n",
    "    print(\"\\n--- ANNOTATED TEXT ---\")\n",
    "    \n",
    "  \n",
    "    display_text = text[:max_tokens]\n",
    "    display_tags = tags[:max_tokens]\n",
    "    \n",
    "    current_entity = []\n",
    "    current_entity_type = None\n",
    "    \n",
    "    for i, (token, tag) in enumerate(zip(display_text, display_tags)):\n",
    "        if tag.startswith('B-'):\n",
    "            \n",
    "            if current_entity:\n",
    "                entity_text = ' '.join(current_entity)\n",
    "                print(f\"[{entity_text}]({current_entity_type})\", end=' ')\n",
    "                current_entity = []\n",
    "            \n",
    "            \n",
    "            current_entity_type = tag[2:]\n",
    "            current_entity = [token]\n",
    "        \n",
    "        elif tag.startswith('I-'):\n",
    "    \n",
    "            if current_entity_type == tag[2:]:\n",
    "                current_entity.append(token)\n",
    "            else:\n",
    "       \n",
    "                if current_entity:\n",
    "                    entity_text = ' '.join(current_entity)\n",
    "                    print(f\"[{entity_text}]({current_entity_type})\", end=' ')\n",
    "                current_entity_type = tag[2:]\n",
    "                current_entity = [token]\n",
    "        \n",
    "        else:  # tag == 'O'\n",
    "            \n",
    "            if current_entity:\n",
    "                entity_text = ' '.join(current_entity)\n",
    "                print(f\"[{entity_text}]({current_entity_type})\", end=' ')\n",
    "                current_entity = []\n",
    "                current_entity_type = None\n",
    "            \n",
    "          \n",
    "            print(token, end=' ')\n",
    "    \n",
    "\n",
    "    if current_entity:\n",
    "        entity_text = ' '.join(current_entity)\n",
    "        print(f\"[{entity_text}]({current_entity_type})\", end=' ')\n",
    "    \n",
    "    print(\"\\n\")\n",
    "\n",
    "    entities = []\n",
    "    current_entity = []\n",
    "    current_entity_type = None\n",
    "    \n",
    "    for token, tag in zip(text, tags):\n",
    "        if tag.startswith('B-'):\n",
    "            if current_entity:\n",
    "                entities.append((' '.join(current_entity), current_entity_type))\n",
    "                current_entity = []\n",
    "            current_entity_type = tag[2:]\n",
    "            current_entity = [token]\n",
    "        elif tag.startswith('I-') and current_entity_type == tag[2:]:\n",
    "            current_entity.append(token)\n",
    "        else:\n",
    "            if current_entity:\n",
    "                entities.append((' '.join(current_entity), current_entity_type))\n",
    "                current_entity = []\n",
    "                current_entity_type = None\n",
    "    \n",
    "    if current_entity:\n",
    "        entities.append((' '.join(current_entity), current_entity_type))\n",
    "    \n",
    "    if entities:\n",
    "        print(\"--- EXTRACTED ENTITIES ---\")\n",
    "        for entity, entity_type in entities:\n",
    "            print(f\"• {entity_type}: '{entity}'\")\n",
    "    else:\n",
    "        print(\"--- NO ENTITIES FOUND ---\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "\n",
    "print(\"MANUAL REVIEW - Contoh Anotasi:\")\n",
    "for i in range(min(5, len(df))):\n",
    "    display_annotations(df, i, max_tokens=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eb303c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisasi distribusi entity types\n",
    "def visualize_entity_distribution(df):\n",
    "\n",
    "    # Hitung jumlah entities per type\n",
    "    entity_counts = {}\n",
    "    \n",
    "    for idx in range(len(df)):\n",
    "        text = df.iloc[idx]['text']\n",
    "        tags = df.iloc[idx]['text-tags']\n",
    "        \n",
    "        current_entity_type = None\n",
    "        \n",
    "        for token, tag in zip(text, tags):\n",
    "            if tag.startswith('B-'):\n",
    "                entity_type = tag[2:]\n",
    "                if entity_type not in entity_counts:\n",
    "                    entity_counts[entity_type] = 0\n",
    "                entity_counts[entity_type] += 1\n",
    "                current_entity_type = entity_type\n",
    "    \n",
    "    \n",
    "    sorted_entities = sorted(entity_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 12))\n",
    "    \n",
    "    # Plot 1: Horizontal bar chart untuk semua entities\n",
    "    entity_names = [item[0] for item in sorted_entities]\n",
    "    entity_values = [item[1] for item in sorted_entities]\n",
    "    \n",
    "    bars = ax1.barh(range(len(entity_names)), entity_values, color=plt.cm.Set3(np.linspace(0, 1, len(entity_names))))\n",
    "    ax1.set_yticks(range(len(entity_names)))\n",
    "    ax1.set_yticklabels(entity_names, fontsize=10)\n",
    "    ax1.set_xlabel('Jumlah Entities')\n",
    "    ax1.set_title('Distribusi Entity Types dalam Dataset', fontsize=14, fontweight='bold')\n",
    "    ax1.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "\n",
    "    for i, bar in enumerate(bars):\n",
    "        width = bar.get_width()\n",
    "        ax1.text(width + max(entity_values)*0.01, bar.get_y() + bar.get_height()/2, \n",
    "                f'{int(width)}', ha='left', va='center', fontsize=9)\n",
    "    \n",
    "    # Plot 2: Pie chart untuk top 10 entities\n",
    "    top_10 = sorted_entities[:10]\n",
    "    others_count = sum([item[1] for item in sorted_entities[10:]])\n",
    "    \n",
    "    pie_labels = [item[0] for item in top_10]\n",
    "    pie_values = [item[1] for item in top_10]\n",
    "    \n",
    "    if others_count > 0:\n",
    "        pie_labels.append('Others')\n",
    "        pie_values.append(others_count)\n",
    "    \n",
    "    wedges, texts, autotexts = ax2.pie(pie_values, labels=pie_labels, autopct='%1.1f%%', \n",
    "                                       startangle=90, colors=plt.cm.Set3(np.linspace(0, 1, len(pie_values))))\n",
    "    ax2.set_title('Distribusi Top 10 Entity Types', fontsize=14, fontweight='bold')\n",
    "\n",
    "    for text in texts:\n",
    "        text.set_fontsize(10)\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_fontsize(9)\n",
    "        autotext.set_color('white')\n",
    "        autotext.set_weight('bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return entity_counts\n",
    "\n",
    "\n",
    "entity_counts = visualize_entity_distribution(df)\n",
    "print(f\"\\nTotal unique entity types: {len(entity_counts)}\")\n",
    "print(f\"Total entities found: {sum(entity_counts.values())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02968145",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aee1df9",
   "metadata": {},
   "source": [
    "# Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e759f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Loading dataset splits from PUBLIC folder...\")\n",
    "\n",
    "train_ids = pd.read_csv('../../Datasets/PUBLIC/train.ids.csv', header=None, names=['id'])\n",
    "print(f\"Train IDs loaded: {len(train_ids)} documents\")\n",
    "\n",
    "\n",
    "val_ids = pd.read_csv('../../Datasets/PUBLIC/val.ids.csv', header=None, names=['id'])\n",
    "print(f\"Validation IDs loaded: {len(val_ids)} documents\")\n",
    "\n",
    "\n",
    "test_ids = pd.read_csv('../../Datasets/PUBLIC/test.ids.csv', header=None, names=['id'])\n",
    "print(f\"Test IDs loaded: {len(test_ids)} documents\")\n",
    "\n",
    "train_ids_set = set(train_ids['id'].tolist())\n",
    "val_ids_set = set(val_ids['id'].tolist())\n",
    "test_ids_set = set(test_ids['id'].tolist())\n",
    "\n",
    "print(f\"\\nDataset split summary:\")\n",
    "print(f\"Train: {len(train_ids_set)} documents\")\n",
    "print(f\"Validation: {len(val_ids_set)} documents\")\n",
    "print(f\"Test: {len(test_ids_set)} documents\")\n",
    "print(f\"Total: {len(train_ids_set) + len(val_ids_set) + len(test_ids_set)} documents\")\n",
    "\n",
    "# Verifikasi tidak ada overlap\n",
    "overlap_train_val = train_ids_set.intersection(val_ids_set)\n",
    "overlap_train_test = train_ids_set.intersection(test_ids_set)\n",
    "overlap_val_test = val_ids_set.intersection(test_ids_set)\n",
    "\n",
    "print(f\"\\nOverlap check:\")\n",
    "print(f\"Train-Val overlap: {len(overlap_train_val)} documents\")\n",
    "print(f\"Train-Test overlap: {len(overlap_train_test)} documents\")\n",
    "print(f\"Val-Test overlap: {len(overlap_val_test)} documents\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d04a6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Splitting dataframe based on predefined IDs...\")\n",
    "\n",
    "# Filter dataframe berdasarkan ID\n",
    "train_df = df[df['id'].isin(train_ids_set)].copy().reset_index(drop=True)\n",
    "val_df = df[df['id'].isin(val_ids_set)].copy().reset_index(drop=True)\n",
    "test_df = df[df['id'].isin(test_ids_set)].copy().reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nDataFrame splits created:\")\n",
    "print(f\"Train DataFrame: {len(train_df)} documents\")\n",
    "print(f\"Validation DataFrame: {len(val_df)} documents\")\n",
    "print(f\"Test DataFrame: {len(test_df)} documents\")\n",
    "\n",
    "\n",
    "total_split = len(train_df) + len(val_df) + len(test_df)\n",
    "print(f\"\\nOriginal dataset: {len(df)} documents\")\n",
    "print(f\"Split total: {total_split} documents\")\n",
    "\n",
    "if total_split == len(df):\n",
    "    print(\"All documents successfully assigned to splits!\")\n",
    "else:\n",
    "    missing = len(df) - total_split\n",
    "    print(f\"{missing} documents are missing from splits\")\n",
    "    \n",
    "   \n",
    "    all_split_ids = train_ids_set.union(val_ids_set).union(test_ids_set)\n",
    "    original_ids = set(df['id'].tolist())\n",
    "    missing_ids = original_ids - all_split_ids\n",
    "    if missing_ids:\n",
    "        print(f\"Missing IDs: {list(missing_ids)[:10]}...\")  # Show first 10\n",
    "\n",
    "\n",
    "print(f\"\\nVerdict distribution per split:\")\n",
    "print(f\"\\nTrain verdict distribution:\")\n",
    "print(train_df['verdict'].value_counts(normalize=True).round(3))\n",
    "\n",
    "print(f\"\\nValidation verdict distribution:\")\n",
    "print(val_df['verdict'].value_counts(normalize=True).round(3))\n",
    "\n",
    "print(f\"\\nTest verdict distribution:\")\n",
    "print(test_df['verdict'].value_counts(normalize=True).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d5f0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Setting up XLM-RoBERTa tokenizer...\")\n",
    "\n",
    "# Load tokenizer XLM-RoBERTa\n",
    "model_name = \"xlm-roberta-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Loaded tokenizer: {model_name}\")\n",
    "print(f\"Vocab size: {tokenizer.vocab_size}\")\n",
    "print(f\"Special tokens: {tokenizer.special_tokens_map}\")\n",
    "print(f\"CLS token: '{tokenizer.cls_token}' (ID: {tokenizer.cls_token_id})\")\n",
    "print(f\"SEP token: '{tokenizer.sep_token}' (ID: {tokenizer.sep_token_id})\")\n",
    "print(f\"PAD token: '{tokenizer.pad_token}' (ID: {tokenizer.pad_token_id})\")\n",
    "print(f\"UNK token: '{tokenizer.unk_token}' (ID: {tokenizer.unk_token_id})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532fbd1e",
   "metadata": {},
   "source": [
    "Token CLS (classification token) biasanya ditempatkan di awal urutan dan digunakan untuk tugas klasifikasi. Token SEP (separator token) memisahkan segmen input teks yang berbeda. Token PAD digunakan untuk membuat urutan dengan panjang berbeda menjadi seragam untuk pemrosesan batch. Terakhir, token UNK (unknown token) mewakili kata-kata yang tidak ada dalam vocabulary model. Setiap token ditampilkan dengan representasi string dan ID numerik yang sesuai, yang merupakan cara model sebenarnya memproses penanda khusus ini secara internal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11000830",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample_tokens = df.iloc[0]['text'][:100]\n",
    "print(f\"\\n Testing tokenization on sample:\")\n",
    "print(f\"Original tokens: {sample_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d645181",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Tokenize dengan berbagai parameter\n",
    "test_encoding = tokenizer(\n",
    "    sample_tokens,\n",
    "    is_split_into_words=True,\n",
    "    add_special_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    "    return_attention_mask=True\n",
    ")\n",
    "\n",
    "print(f\"\\nTokenization result:\")\n",
    "print(f\"Input IDs: {test_encoding['input_ids']}\")\n",
    "print(f\"Tokens: {tokenizer.convert_ids_to_tokens(test_encoding['input_ids'])}\")\n",
    "print(f\"Word IDs: {test_encoding.word_ids()}\")\n",
    "print(f\"Attention mask: {test_encoding['attention_mask']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5a52d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"mapping NER...\")\n",
    "\n",
    "def create_label_mapping(df):\n",
    "\n",
    "    all_labels = set()\n",
    "    \n",
    "    for tags in df['text-tags']:\n",
    "        all_labels.update(tags)\n",
    "    \n",
    "    sorted_labels = sorted(list(all_labels))\n",
    "    \n",
    "\n",
    "    if 'O' in sorted_labels:\n",
    "        sorted_labels.remove('O')\n",
    "        sorted_labels = ['O'] + sorted_labels\n",
    "    \n",
    "    # Buat mapping\n",
    "    label2id = {label: idx for idx, label in enumerate(sorted_labels)}\n",
    "    id2label = {idx: label for label, idx in label2id.items()}\n",
    "    \n",
    "    return label2id, id2label, sorted_labels\n",
    "\n",
    "# Create mappings\n",
    "label2id, id2label, all_labels = create_label_mapping(df)\n",
    "\n",
    "print(f\"\\nLabel mapping created:\")\n",
    "print(f\"Total unique labels: {len(all_labels)}\")\n",
    "print(f\"Label 'O' has ID: {label2id['O']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3606fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0fbab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nSample label mapping (first 15):\")\n",
    "for i, (label, idx) in enumerate(list(label2id.items())[:15]):\n",
    "    print(f\"{idx:2d}: {label}\")\n",
    "\n",
    "if len(label2id) > 15:\n",
    "    print(f\"... dan {len(label2id)-15} labels lainnya\")\n",
    "\n",
    "# Analisis distribusi entity types (tanpa B-/I- prefix)\n",
    "entity_types = set()\n",
    "for label in all_labels:\n",
    "    if label != 'O':\n",
    "        entity_type = label.split('-', 1)[1] if '-' in label else label\n",
    "        entity_types.add(entity_type)\n",
    "\n",
    "print(f\"\\nEntity types found: {len(entity_types)}\")\n",
    "print(f\"Entity types: {sorted(list(entity_types))[:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b17913",
   "metadata": {},
   "source": [
    "Tujuan Utama\n",
    "\n",
    "Fungsi ini menyelesaikan masalah kompleks dalam NER: bagaimana menyelaraskan label entitas dengan subword tokens yang dihasilkan oleh tokenizer modern seperti XLM-RoBERTa.\n",
    "\n",
    "Masalah yang Dipecahkan\n",
    "\n",
    "Ketika Anda memiliki data seperti:\n",
    "\n",
    "Tokens: [\"John\", \"Smith\", \"bekerja\", \"di\", \"Jakarta\"]\n",
    "\n",
    "Labels: [\"B-PER\", \"I-PER\", \"O\", \"O\", \"B-LOC\"]\n",
    "\n",
    "Tokenizer sering memecah kata menjadi subword:\n",
    "\n",
    "\"Jakarta\" → [\"Jak\", \"##arta\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b6823f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize_and_align_labels(examples_df, tokenizer, label2id, max_length=512):\n",
    "    \"\"\"\n",
    "    Tokenisasi teks dan alignment labels untuk subword tokens\n",
    "    Args:\n",
    "        examples_df: DataFrame dengan kolom 'text' dan 'text-tags'\n",
    "        tokenizer: HuggingFace tokenizer\n",
    "        label2id: Dictionary mapping label ke ID\n",
    "        max_length: Maksimum panjang sequence\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries dengan keys: input_ids, attention_mask, labels\n",
    "    \"\"\"\n",
    "    tokenized_inputs = []\n",
    "    \n",
    "    print(f\"Tokenizing {len(examples_df)} documents...\")\n",
    "    \n",
    "    for idx in range(len(examples_df)):\n",
    "        if idx % 100 == 0: \n",
    "            print(f\"  Processed {idx}/{len(examples_df)} documents...\")\n",
    "            \n",
    "        tokens = examples_df.iloc[idx]['text']\n",
    "        labels = examples_df.iloc[idx]['text-tags']\n",
    "        \n",
    "        \n",
    "        if len(tokens) != len(labels):\n",
    "            print(f\"Length mismatch at index {idx}: tokens={len(tokens)}, labels={len(labels)}\")\n",
    "            continue\n",
    "        \n",
    "        # Tokenisasi dengan preserving word boundaries\n",
    "        try:\n",
    "            tokenized_input = tokenizer(\n",
    "                tokens,\n",
    "                is_split_into_words=True,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=max_length,\n",
    "                return_offsets_mapping=True,\n",
    "                return_attention_mask=True,\n",
    "                return_tensors=None \n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Tokenization error at index {idx}: {e}\")\n",
    "            continue\n",
    "        \n",
    "       \n",
    "        word_ids = tokenized_input.word_ids()\n",
    "        aligned_labels = []\n",
    "        \n",
    "        previous_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                # Special tokens (CLS, SEP, PAD)\n",
    "                aligned_labels.append(-100)  \n",
    "            elif word_idx != previous_word_idx:\n",
    "                \n",
    "                if word_idx < len(labels):  # Safety check\n",
    "                    aligned_labels.append(label2id[labels[word_idx]])\n",
    "                else:\n",
    "                    aligned_labels.append(-100)\n",
    "            else:\n",
    "                # Subsequent subword tokens dari word yang sama\n",
    "                # Set ke -100 (ignore) untuk menghindari duplikasi loss\n",
    "                aligned_labels.append(-100)\n",
    "            \n",
    "            previous_word_idx = word_idx\n",
    "        \n",
    "        # Pastikan panjang aligned_labels sama dengan input_ids\n",
    "        if len(aligned_labels) != len(tokenized_input['input_ids']):\n",
    "            print(f\"Alignment error at index {idx}\")\n",
    "            continue\n",
    "        \n",
    "        tokenized_input['labels'] = aligned_labels\n",
    "        \n",
    "        # Remove offset_mapping karena tidak diperlukan untuk training\n",
    "        del tokenized_input['offset_mapping']\n",
    "        \n",
    "        tokenized_inputs.append(tokenized_input)\n",
    "    \n",
    "    print(f\"Tokenization completed: {len(tokenized_inputs)} documents processed\")\n",
    "    return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce8030d",
   "metadata": {},
   "source": [
    "Logika alignment:\n",
    "\n",
    "Special tokens (CLS, SEP, PAD) → -100 (diabaikan dalam loss calculation)\n",
    "\n",
    "First subword dari setiap kata → mendapat label asli\n",
    "\n",
    "Subsequent subwords → -100 (menghindari duplikasi loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d2680f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Test pada 3 dokumen pertama dari train set\n",
    "sample_df = train_df.head(3)\n",
    "MAX_LENGTH = 512  # Set maximum length\n",
    "\n",
    "tokenized_samples = tokenize_and_align_labels(sample_df, tokenizer, label2id, MAX_LENGTH)\n",
    "\n",
    "print(f\"\\nTokenization test results:\")\n",
    "print(f\"Input samples: {len(sample_df)}\")\n",
    "print(f\"Output samples: {len(tokenized_samples)}\")\n",
    "\n",
    "if tokenized_samples:\n",
    "    # Analisis sample pertama\n",
    "    sample = tokenized_samples[1]\n",
    "    print(f\"\\nSample 0 analysis:\")\n",
    "    print(f\"Input IDs length: {len(sample['input_ids'])}\")\n",
    "    print(f\"Labels length: {len(sample['labels'])}\")\n",
    "    print(f\"Attention mask length: {len(sample['attention_mask'])}\")\n",
    "    print(f\"Max length setting: {MAX_LENGTH}\")\n",
    "    \n",
    "    # Decode beberapa tokens untuk verifikasi\n",
    "    tokens = tokenizer.convert_ids_to_tokens(sample['input_ids'][:50])\n",
    "    labels = sample['labels'][:50]\n",
    "    \n",
    "    print(f\"\\nFirst 50 tokens and labels:\")\n",
    "    print(f\"{'Token':<15} {'Label ID':<8} {'Label Name':<25}\")\n",
    "    print(\"-\" * 50)\n",
    "    for token, label_id in zip(tokens, labels):\n",
    "        label_name = id2label[label_id] if label_id != -100 else \"IGNORE\"\n",
    "        print(f\"{token:<15} {label_id:<8} {label_name:<25}\")\n",
    "    \n",
    "    # Statistik labels\n",
    "    all_label_ids = []\n",
    "    for sample in tokenized_samples:\n",
    "        all_label_ids.extend([l for l in sample['labels'] if l != -100])\n",
    "    \n",
    "    print(f\"\\nLabel statistics in test samples:\")\n",
    "    from collections import Counter\n",
    "    label_counts = Counter(all_label_ids)\n",
    "    for label_id, count in label_counts.most_common(10):\n",
    "        print(f\"{id2label[label_id]:<25}: {count}\")\n",
    "\n",
    "else:\n",
    "    print(\"No samples were successfully tokenized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b582f243",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc2cf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisasi semua dataset splits\n",
    "print(\"Starting full tokenization for all dataset splits...\")\n",
    "print(\"This may take several minutes depending on dataset size...\")\n",
    "\n",
    "# Tentukan max_length berdasarkan analisis panjang dokumen\n",
    "def analyze_document_lengths(df_list, names):\n",
    "    print(\"\\nAnalyzing document lengths...\")\n",
    "    for df, name in zip(df_list, names):\n",
    "        lengths = [len(text) for text in df['text']]\n",
    "        print(f\"\\n{name} set:\")\n",
    "        print(f\"  Mean length: {np.mean(lengths):.1f} tokens\")\n",
    "        print(f\"  Median length: {np.median(lengths):.1f} tokens\")\n",
    "        print(f\"  Max length: {max(lengths)} tokens\")\n",
    "        print(f\"  95th percentile: {np.percentile(lengths, 95):.1f} tokens\")\n",
    "        print(f\"  99th percentile: {np.percentile(lengths, 99):.1f} tokens\")\n",
    "\n",
    "analyze_document_lengths([train_df, val_df, test_df], [\"Train\", \"Validation\", \"Test\"])\n",
    "\n",
    "# Set MAX_LENGTH berdasarkan analisis (bisa disesuaikan)\n",
    "MAX_LENGTH = 512\n",
    "print(f\"\\nUsing MAX_LENGTH = {MAX_LENGTH}\")\n",
    "print(f\"Note: Documents longer than {MAX_LENGTH} will be truncated\")\n",
    "\n",
    "# Tokenisasi train set\n",
    "print(f\"\\nTokenizing training set ({len(train_df)} documents)...\")\n",
    "train_tokenized = tokenize_and_align_labels(train_df, tokenizer, label2id, MAX_LENGTH)\n",
    "\n",
    "# Tokenisasi validation set\n",
    "print(f\"\\nTokenizing validation set ({len(val_df)} documents)...\")\n",
    "val_tokenized = tokenize_and_align_labels(val_df, tokenizer, label2id, MAX_LENGTH)\n",
    "\n",
    "# Tokenisasi test set\n",
    "print(f\"\\nTokenizing test set ({len(test_df)} documents)...\")\n",
    "test_tokenized = tokenize_and_align_labels(test_df, tokenizer, label2id, MAX_LENGTH)\n",
    "\n",
    "print(f\"\\nTokenization completed for all splits!\")\n",
    "print(f\"Final tokenized dataset sizes:\")\n",
    "print(f\"  Train: {len(train_tokenized)} samples\")\n",
    "print(f\"  Validation: {len(val_tokenized)} samples\")\n",
    "print(f\"  Test: {len(test_tokenized)} samples\")\n",
    "print(f\"  Total: {len(train_tokenized) + len(val_tokenized) + len(test_tokenized)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc0514e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def analyze_tokenized_data(tokenized_data, dataset_name):\n",
    "\n",
    "    print(f\"\\n{dataset_name} tokenization statistics:\")\n",
    "    \n",
    "    if not tokenized_data:\n",
    "        print(\"  No data to analyze\")\n",
    "        return\n",
    "    \n",
    "    # Analisis panjang sequence\n",
    "    input_lengths = [len(sample['input_ids']) for sample in tokenized_data]\n",
    "    actual_lengths = [sum(sample['attention_mask']) for sample in tokenized_data]  # Non-padded length\n",
    "    \n",
    "    print(f\"    Sequence lengths:\")\n",
    "    print(f\"    Max length (with padding): {max(input_lengths)}\")\n",
    "    print(f\"    Actual length - Mean: {np.mean(actual_lengths):.1f}\")\n",
    "    print(f\"    Actual length - Median: {np.median(actual_lengths):.1f}\")\n",
    "    print(f\"    Actual length - Max: {max(actual_lengths)}\")\n",
    "    print(f\"    Actual length - Min: {min(actual_lengths)}\")\n",
    "    \n",
    "    # Hitung berapa dokumen yang kena truncation\n",
    "    truncated = sum(1 for length in actual_lengths if length == MAX_LENGTH)\n",
    "    print(f\"    Truncated documents: {truncated} ({truncated/len(tokenized_data)*100:.1f}%)\")\n",
    "    \n",
    "    # Analisis distribusi labels\n",
    "    all_labels = []\n",
    "    ignored_tokens = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    for sample in tokenized_data:\n",
    "        for label in sample['labels']:\n",
    "            total_tokens += 1\n",
    "            if label == -100:\n",
    "                ignored_tokens += 1\n",
    "            else:\n",
    "                all_labels.append(label)\n",
    "    \n",
    "    print(f\"    Label statistics:\")\n",
    "    print(f\"    Total tokens: {total_tokens:,}\")\n",
    "    print(f\"    Ignored tokens: {ignored_tokens:,} ({ignored_tokens/total_tokens*100:.1f}%)\")\n",
    "    print(f\"    Valid labels: {len(all_labels):,} ({len(all_labels)/total_tokens*100:.1f}%)\")\n",
    "    \n",
    "    # Distribusi top labels\n",
    "    if all_labels:\n",
    "        label_counts = Counter(all_labels)\n",
    "        print(f\"    Top 10 labels:\")\n",
    "        for label_id, count in label_counts.most_common(10):\n",
    "            label_name = id2label[label_id]\n",
    "            percentage = count / len(all_labels) * 100\n",
    "            print(f\"      {label_name:<25}: {count:>6,} ({percentage:>5.1f}%)\")\n",
    "    \n",
    "    return {\n",
    "        'total_samples': len(tokenized_data),\n",
    "        'avg_length': np.mean(actual_lengths),\n",
    "        'max_length': max(actual_lengths),\n",
    "        'truncated_count': truncated,\n",
    "        'total_tokens': total_tokens,\n",
    "        'valid_labels': len(all_labels),\n",
    "        'ignored_tokens': ignored_tokens\n",
    "    }\n",
    "\n",
    "# Analisis semua splits\n",
    "print(\"Analyzing tokenized datasets...\")\n",
    "\n",
    "train_stats = analyze_tokenized_data(train_tokenized, \"Training\")\n",
    "val_stats = analyze_tokenized_data(val_tokenized, \"Validation\")\n",
    "test_stats = analyze_tokenized_data(test_tokenized, \"Test\")\n",
    "\n",
    "# Summary total\n",
    "print(f\"\\nOverall Summary:\")\n",
    "total_samples = train_stats['total_samples'] + val_stats['total_samples'] + test_stats['total_samples']\n",
    "total_tokens = train_stats['total_tokens'] + val_stats['total_tokens'] + test_stats['total_tokens']\n",
    "total_valid_labels = train_stats['valid_labels'] + val_stats['valid_labels'] + test_stats['valid_labels']\n",
    "\n",
    "print(f\"  Total samples: {total_samples:,}\")\n",
    "print(f\"  Total tokens: {total_tokens:,}\")\n",
    "print(f\"  Total valid labels: {total_valid_labels:,}\")\n",
    "print(f\"  Average tokens per sample: {total_tokens/total_samples:.1f}\")\n",
    "print(f\"  Label density: {total_valid_labels/total_tokens*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbc950b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save hasil preprocessing untuk training\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "\n",
    "print(\"Saving preprocessing results...\")\n",
    "\n",
    "# Buat direktori untuk menyimpan hasil preprocessing\n",
    "output_dir = \"./results\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save tokenized data\n",
    "print(\"  Saving tokenized datasets...\")\n",
    "with open(os.path.join(output_dir, 'train_tokenized.pkl'), 'wb') as f:\n",
    "    pickle.dump(train_tokenized, f)\n",
    "    \n",
    "with open(os.path.join(output_dir, 'val_tokenized.pkl'), 'wb') as f:\n",
    "    pickle.dump(val_tokenized, f)\n",
    "    \n",
    "with open(os.path.join(output_dir, 'test_tokenized.pkl'), 'wb') as f:\n",
    "    pickle.dump(test_tokenized, f)\n",
    "\n",
    "# Save metadata untuk training\n",
    "metadata = {\n",
    "    'model_name': model_name,\n",
    "    'max_length': MAX_LENGTH,\n",
    "    'num_labels': len(label2id),\n",
    "    'label2id': label2id,\n",
    "    'id2label': {int(k): v for k, v in id2label.items()},  # JSON serializable\n",
    "    'entity_types': sorted(list(entity_types)),\n",
    "    'dataset_splits': {\n",
    "        'train_size': len(train_tokenized),\n",
    "        'val_size': len(val_tokenized),\n",
    "        'test_size': len(test_tokenized)\n",
    "    },\n",
    "    'statistics': {\n",
    "        'train': train_stats,\n",
    "        'val': val_stats,\n",
    "        'test': test_stats\n",
    "    },\n",
    "    'preprocessing_date': pd.Timestamp.now().isoformat()\n",
    "}\n",
    "\n",
    "print(\"  Saving metadata...\")\n",
    "with open(os.path.join(output_dir, 'preprocessing_metadata.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Save original dataframes untuk referensi\n",
    "print(\"  Saving original DataFrames...\")\n",
    "train_df.to_csv(os.path.join(output_dir, 'train_df.csv'), index=False)\n",
    "val_df.to_csv(os.path.join(output_dir, 'val_df.csv'), index=False)\n",
    "test_df.to_csv(os.path.join(output_dir, 'test_df.csv'), index=False)\n",
    "\n",
    "# Save label mappings secara terpisah\n",
    "print(\"  Saving label mappings...\")\n",
    "with open(os.path.join(output_dir, 'label2id.json'), 'w') as f:\n",
    "    json.dump(label2id, f, indent=2)\n",
    "    \n",
    "with open(os.path.join(output_dir, 'id2label.json'), 'w') as f:\n",
    "    json.dump({int(k): v for k, v in id2label.items()}, f, indent=2)\n",
    "\n",
    "print(f\"\\nPreprocessing results saved to '{output_dir}' directory:\")\n",
    "print(f\"  Files saved:\")\n",
    "for filename in os.listdir(output_dir):\n",
    "    file_path = os.path.join(output_dir, filename)\n",
    "    file_size = os.path.getsize(file_path) / (1024*1024)  # MB\n",
    "    print(f\"    {filename} ({file_size:.1f} MB)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820bf7d8",
   "metadata": {},
   "source": [
    "### Output Files in `./results/`:\n",
    "\n",
    "- `train_tokenized.pkl` - Training data\n",
    "- `val_tokenized.pkl` - Validation data\n",
    "- `test_tokenized.pkl` - Test data\n",
    "- `preprocessing_metadata.json` - Complete metadata\n",
    "- `label2id.json` & `id2label.json` - Label mappings\n",
    "- `train_df.csv`, `val_df.csv`, `test_df.csv` - Original splits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce40ffaf",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d256a8",
   "metadata": {},
   "source": [
    "# Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13380ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data and setup XLM-RoBERTa model\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification\n",
    ")\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "from seqeval.metrics import f1_score, precision_score, recall_score, classification_report as seq_classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd22af78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"NER Model\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "\n",
    "# Load preprocessed data\n",
    "preprocessed_dir = './results'\n",
    "\n",
    "print(\"\\nLoading preprocessed data...\")\n",
    "\n",
    "# Load tokenized datasets\n",
    "with open(os.path.join(preprocessed_dir, 'train_tokenized.pkl'), 'rb') as f:\n",
    "    train_dataset = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(preprocessed_dir, 'val_tokenized.pkl'), 'rb') as f:\n",
    "    val_dataset = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(preprocessed_dir, 'test_tokenized.pkl'), 'rb') as f:\n",
    "    test_dataset = pickle.load(f)\n",
    "\n",
    "# Load metadata and label mappings\n",
    "with open(os.path.join(preprocessed_dir, 'preprocessing_metadata.json'), 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "    \n",
    "with open(os.path.join(preprocessed_dir, 'label2id.json'), 'r') as f:\n",
    "    label2id = json.load(f)\n",
    "    \n",
    "with open(os.path.join(preprocessed_dir, 'id2label.json'), 'r') as f:\n",
    "    id2label = {int(k): v for k, v in json.load(f).items()}\n",
    "\n",
    "print(f\"Loaded {len(train_dataset)} training samples\")\n",
    "print(f\"Loaded {len(val_dataset)} validation samples\")\n",
    "print(f\"Loaded {len(test_dataset)} test samples\")\n",
    "print(f\"Number of labels: {len(label2id)}\")\n",
    "print(f\"Model name: {metadata['model_name']}\")\n",
    "print(f\"Max length: {metadata['max_length']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd04694",
   "metadata": {},
   "source": [
    "Mengkonversi list Python menjadi torch.tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff1c260",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Custom Dataset class for NER\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, tokenized_data):\n",
    "        self.data = tokenized_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        return {\n",
    "            'input_ids': torch.tensor(item['input_ids'], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(item['attention_mask'], dtype=torch.long),\n",
    "            'labels': torch.tensor(item['labels'], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create dataset objects\n",
    "train_torch_dataset = NERDataset(train_dataset)\n",
    "val_torch_dataset = NERDataset(val_dataset)\n",
    "test_torch_dataset = NERDataset(test_dataset)\n",
    "\n",
    "print(f\"\\nCreated PyTorch datasets\")\n",
    "print(f\"  Training: {len(train_torch_dataset)} samples\")\n",
    "print(f\"  Validation: {len(val_torch_dataset)} samples\")\n",
    "print(f\"  Test: {len(test_torch_dataset)} samples\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = metadata['model_name']  # 'xlm-roberta-large'\n",
    "print(f\"\\nLoading {model_name} for token classification...\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(f\"Tokenizer loaded - Vocab size: {len(tokenizer)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e96a85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_torch_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc8858d",
   "metadata": {},
   "source": [
    "num_labels: Jumlah class NER yang akan diprediksi\n",
    "\n",
    "id2label & label2id: Mapping untuk interpretasi output\n",
    "\n",
    "ignore_mismatched_sizes: Mengabaikan perbedaan ukuran classification head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccff45f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load model for token classification\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(label2id),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "print(f\"  Model loaded and moved to {device}\")\n",
    "print(f\"  Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"  Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "print(f\"  Number of labels: {model.num_labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b707ade0",
   "metadata": {},
   "source": [
    "menggabungkan multiple samples menjadi satu batch untuk training, untuk pararel processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212be850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator for token classification\n",
    "data_collator = DataCollatorForTokenClassification(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0612cb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    # Get predictions (argmax of logits)\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    for prediction, label in zip(predictions, labels):\n",
    "        valid_indices = label != -100\n",
    "\n",
    "        pred_labels = [id2label[p] for p, valid in zip(prediction, valid_indices) if valid]\n",
    "        true_label_list = [id2label[l] for l, valid in zip(label, valid_indices) if valid]\n",
    "\n",
    "        if len(pred_labels) > 0 and len(true_label_list) > 0:\n",
    "            true_predictions.append(pred_labels)\n",
    "            true_labels.append(true_label_list)\n",
    "\n",
    "    # Compute seqeval metrics\n",
    "    precision = precision_score(true_labels, true_predictions)\n",
    "    recall = recall_score(true_labels, true_predictions)\n",
    "    f1 = f1_score(true_labels, true_predictions)\n",
    "\n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10c7e28",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2621f4",
   "metadata": {},
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c1d872",
   "metadata": {},
   "source": [
    "- 3 epochs - Cukup untuk fine-tuning pre-trained model tanpa overfitting\n",
    "- Small batch size (4) - XLM-RoBERTa-large membutuhkan banyak memory, batch kecil mencegah OOM\n",
    "- Larger eval batch (6) - Inference lebih memory-efficient, bisa gunakan batch lebih besar\n",
    "- 2e-5 - Learning rate optimal untuk fine-tuning transformer (tidak terlalu aggressive)\n",
    "- Weight decay 0.01 - Mencegah overfitting dengan L2 regularization\n",
    "- Warmup 10% - Gradual increase LR untuk stable training start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108a1b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "output_dir = './models/xlm_roberta_ner_results'\n",
    "logging_dir = './models/cache/xlm_roberta_ner_logs'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=6,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    \n",
    "    # Logging and evaluation\n",
    "    logging_dir=logging_dir,\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    save_total_limit=3,\n",
    "    \n",
    "    # Other settings\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    report_to=None, \n",
    "    \n",
    "    # Performance\n",
    "    dataloader_pin_memory=False,\n",
    "    remove_unused_columns=False,\n",
    "    \n",
    "    # For debugging\n",
    "    # max_steps=100,\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Training arguments configured:\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Train batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Eval batch size: {training_args.per_device_eval_batch_size}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171b98b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9636a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_torch_dataset,\n",
    "    eval_dataset=val_torch_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining Setup Summary:\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Total labels: {len(label2id)}\")\n",
    "print(f\"Training samples: {len(train_torch_dataset):,}\")\n",
    "print(f\"Validation samples: {len(val_torch_dataset):,}\")\n",
    "print(f\"Test samples: {len(test_torch_dataset):,}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"Batch size (train): {training_args.per_device_train_batch_size}\")\n",
    "print(f\"Learning rate: {training_args.learning_rate}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aff842a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up memory\n",
    "import gc\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"\\nGPU Memory after cleanup: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78101458",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7b0ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting Model Training.\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Train the model\n",
    "    train_result = trainer.train()\n",
    "    \n",
    "    # Training completed\n",
    "    end_time = time.time()\n",
    "    training_duration = end_time - start_time\n",
    "    \n",
    "    print(f\"\\nTraining completed successfully!\")\n",
    "    print(f\"Training duration: {training_duration/60:.2f} minutes\")\n",
    "    print(f\"Final training loss: {train_result.training_loss:.4f}\")\n",
    "    \n",
    "    # Save the model\n",
    "    trainer.save_model()\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "    print(f\"\\nModel and tokenizer saved to: {output_dir}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nTraining failed with error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9ae30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nEvaluating on validation set...\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "try:\n",
    "    eval_results = trainer.evaluate()\n",
    "    \n",
    "    print(f\"\\n✓ Validation Results:\")\n",
    "    print(f\"  Validation Loss: {eval_results['eval_loss']:.4f}\")\n",
    "    print(f\"  Precision: {eval_results['eval_precision']:.4f}\")\n",
    "    print(f\"  Recall: {eval_results['eval_recall']:.4f}\")\n",
    "    print(f\"  F1-Score: {eval_results['eval_f1']:.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nValidation failed with error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f80658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed evaluation on test set\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "try:\n",
    "    # Get predictions on test set\n",
    "    test_predictions = trainer.predict(test_torch_dataset)\n",
    "    \n",
    "    # Process predictions\n",
    "    predictions = np.argmax(test_predictions.predictions, axis=2)\n",
    "    labels = test_predictions.label_ids\n",
    "    \n",
    "    # Convert to label names (excluding special tokens)\n",
    "    true_predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    for prediction, label in zip(predictions, labels):\n",
    "        valid_indices = label != -100\n",
    "        \n",
    "        pred_labels = [id2label[p] for p, valid in zip(prediction, valid_indices) if valid]\n",
    "        true_label_list = [id2label[l] for l, valid in zip(label, valid_indices) if valid]\n",
    "        \n",
    "        if len(pred_labels) > 0 and len(true_label_list) > 0:\n",
    "            true_predictions.append(pred_labels)\n",
    "            true_labels.append(true_label_list)\n",
    "    \n",
    "    # Compute detailed metrics\n",
    "    precision = precision_score(true_labels, true_predictions)\n",
    "    recall = recall_score(true_labels, true_predictions)\n",
    "    f1 = f1_score(true_labels, true_predictions)\n",
    "    \n",
    "    print(f\"\\n✓ Test Set Results:\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}\")\n",
    "    print(f\"  F1-Score: {f1:.4f}\")\n",
    "    \n",
    "    # Detailed classification report\n",
    "    print(f\"\\nDetailed Classification Report:\")\n",
    "    print(\"=\" * 60)\n",
    "    report = seq_classification_report(true_labels, true_predictions, digits=4)\n",
    "    print(report)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Test evaluation failed with error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaec02c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entity-wise performance analysis\n",
    "print(\"\\nEntity-wise Performance Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    # Flatten all predictions and labels\n",
    "    all_true_labels = []\n",
    "    all_pred_labels = []\n",
    "    \n",
    "    for true_seq, pred_seq in zip(true_labels, true_predictions):\n",
    "        all_true_labels.extend(true_seq)\n",
    "        all_pred_labels.extend(pred_seq)\n",
    "    \n",
    "    # Get unique entity types (non-O labels)\n",
    "    entity_types = set()\n",
    "    for label in all_true_labels + all_pred_labels:\n",
    "        if label != 'O' and not label.startswith('I-'):\n",
    "            if label.startswith('B-'):\n",
    "                entity_types.add(label[2:])\n",
    "            else:\n",
    "                entity_types.add(label)\n",
    "    \n",
    "    entity_types = sorted(entity_types)\n",
    "    \n",
    "    print(f\"Found {len(entity_types)} entity types for analysis:\")\n",
    "    for i, entity_type in enumerate(entity_types, 1):\n",
    "        print(f\"  {i:2d}. {entity_type}\")\n",
    "    \n",
    "    # Count entities per type\n",
    "    true_entity_counts = Counter()\n",
    "    pred_entity_counts = Counter()\n",
    "    \n",
    "    # Extract entities from sequences\n",
    "    def extract_entities(labels):\n",
    "        entities = []\n",
    "        current_entity = None\n",
    "        current_type = None\n",
    "        start_idx = None\n",
    "        \n",
    "        for idx, label in enumerate(labels):\n",
    "            if label.startswith('B-'):\n",
    "                # Save previous entity if exists\n",
    "                if current_entity is not None:\n",
    "                    entities.append((start_idx, idx-1, current_type))\n",
    "                # Start new entity\n",
    "                current_type = label[2:]\n",
    "                current_entity = label\n",
    "                start_idx = idx\n",
    "            elif label.startswith('I-') and current_entity is not None:\n",
    "                # Continue current entity\n",
    "                current_entity += ' ' + label\n",
    "            else:  # O tag\n",
    "                # End current entity if exists\n",
    "                if current_entity is not None:\n",
    "                    entities.append((start_idx, idx-1, current_type))\n",
    "                    current_entity = None\n",
    "                    current_type = None\n",
    "                    start_idx = None\n",
    "        \n",
    "        # Handle entity at end of sequence\n",
    "        if current_entity is not None:\n",
    "            entities.append((start_idx, len(labels)-1, current_type))\n",
    "        \n",
    "        return entities\n",
    "    \n",
    "    # Extract entities from all sequences\n",
    "    for true_seq, pred_seq in zip(true_labels, true_predictions):\n",
    "        true_entities = extract_entities(true_seq)\n",
    "        pred_entities = extract_entities(pred_seq)\n",
    "        \n",
    "        for _, _, entity_type in true_entities:\n",
    "            true_entity_counts[entity_type] += 1\n",
    "        \n",
    "        for _, _, entity_type in pred_entities:\n",
    "            pred_entity_counts[entity_type] += 1\n",
    "    \n",
    "    # Display entity counts\n",
    "    print(f\"\\nEntity Counts Comparison:\")\n",
    "    print(f\"{'Entity Type':<20} {'True':<8} {'Predicted':<10} {'Difference':<10}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for entity_type in entity_types:\n",
    "        true_count = true_entity_counts.get(entity_type, 0)\n",
    "        pred_count = pred_entity_counts.get(entity_type, 0)\n",
    "        diff = pred_count - true_count\n",
    "        \n",
    "        print(f\"{entity_type:<20} {true_count:<8} {pred_count:<10} {diff:+<10}\")\n",
    "    \n",
    "    total_true = sum(true_entity_counts.values())\n",
    "    total_pred = sum(pred_entity_counts.values())\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'TOTAL':<20} {total_true:<8} {total_pred:<10} {total_pred-total_true:+<10}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Entity analysis failed with error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf42eaa3",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc91ce4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference example\n",
    "print(\"\\nTesting Model Inference\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "try:\n",
    "    # Example Indonesian legal text for testing\n",
    "    test_text = \"Majelis Hakim yang diketuai oleh Budi Santoso telah memutuskan bahwa terdakwa Ahmad Rahman terbukti melakukan tindak pidana korupsi dengan putusan nomor 123/Pid.Sus-TPK/2023/PN.Jkt.Pst.\"\n",
    "    \n",
    "    print(f\"Test text: {test_text}\")\n",
    "    print(f\"\\nTokenizing and predicting...\")\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        test_text,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Move to device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Predict\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.argmax(outputs.logits, dim=2)\n",
    "    \n",
    "    # Decode predictions\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "    predicted_labels = [id2label[pred.item()] for pred in predictions[0]]\n",
    "    \n",
    "    print(f\"\\nToken-Label Predictions:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for token, label in zip(tokens, predicted_labels):\n",
    "        if token not in ['<s>', '</s>', '<pad>']:\n",
    "            # Clean up token display\n",
    "            clean_token = token.replace('▁', ' ').strip()\n",
    "            if clean_token:\n",
    "                print(f\"{clean_token:<20} -> {label}\")\n",
    "    \n",
    "    # Extract entities\n",
    "    entities = []\n",
    "    current_entity = []\n",
    "    current_label = None\n",
    "    \n",
    "    for token, label in zip(tokens, predicted_labels):\n",
    "        if token in ['<s>', '</s>', '<pad>']:\n",
    "            continue\n",
    "            \n",
    "        clean_token = token.replace('▁', ' ').strip()\n",
    "        if not clean_token:\n",
    "            continue\n",
    "            \n",
    "        if label.startswith('B-'):\n",
    "            # Save previous entity\n",
    "            if current_entity:\n",
    "                entities.append((' '.join(current_entity).strip(), current_label[2:]))\n",
    "            # Start new entity\n",
    "            current_entity = [clean_token]\n",
    "            current_label = label\n",
    "        elif label.startswith('I-') and current_label and label[2:] == current_label[2:]:\n",
    "            # Continue entity\n",
    "            current_entity.append(clean_token)\n",
    "        else:\n",
    "            # End current entity\n",
    "            if current_entity:\n",
    "                entities.append((' '.join(current_entity).strip(), current_label[2:]))\n",
    "            current_entity = []\n",
    "            current_label = None\n",
    "    \n",
    "    # Add final entity if exists\n",
    "    if current_entity:\n",
    "        entities.append((' '.join(current_entity).strip(), current_label[2:]))\n",
    "    \n",
    "    print(f\"\\nExtracted Entities:\")\n",
    "    print(\"-\" * 40)\n",
    "    if entities:\n",
    "        for entity_text, entity_type in entities:\n",
    "            print(f\"{entity_type:<15}: {entity_text}\")\n",
    "    else:\n",
    "        print(\"No entities detected\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\nInference failed with error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8517a1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"XLM-RoBERTa Indonesian Legal NER Model - TRAINING COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n FINAL RESULTS SUMMARY:\")\n",
    "print(f\"  Model: XLM-RoBERTa Base\")\n",
    "print(f\"  Task: Named Entity Recognition (NER)\")\n",
    "print(f\"  Domain: Indonesian Legal Documents\")\n",
    "print(f\"  Entity Types: {len([l for l in label2id.keys() if l != 'O'])} types\")\n",
    "print(f\"  Training Samples: {len(train_torch_dataset):,}\")\n",
    "print(f\"  Validation Samples: {len(val_torch_dataset):,}\")\n",
    "print(f\"  Test Samples: {len(test_torch_dataset):,}\")\n",
    "\n",
    "if 'eval_results' in locals():\n",
    "    print(f\"\\n VALIDATION PERFORMANCE:\")\n",
    "    print(f\"  F1-Score: {eval_results['eval_f1']:.4f}\")\n",
    "    print(f\"  Precision: {eval_results['eval_precision']:.4f}\")\n",
    "    print(f\"  Recall: {eval_results['eval_recall']:.4f}\")\n",
    "\n",
    "if 'f1' in locals():\n",
    "    print(f\"\\n TEST PERFORMANCE:\")\n",
    "    print(f\"  F1-Score: {f1:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}\")\n",
    "\n",
    "print(f\"\\n MODEL ARTIFACTS:\")\n",
    "print(f\"  Trained model: {output_dir}/\")\n",
    "print(f\"  Preprocessed data: ./preprocessed_data/\")\n",
    "print(f\"  Training logs: {logging_dir}/\")\n",
    "\n",
    "print(f\"\\nThe model is ready for production use!\")\n",
    "print(f\"   You can load it with: AutoModelForTokenClassification.from_pretrained('{output_dir}')\")\n",
    "print(f\"\\n BERT NER Model Training Successfully Completed! 🎉\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13ddb05",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a826bd",
   "metadata": {},
   "source": [
    "# Deployment Preparation\n",
    "\n",
    "Preparing model and preprocessing components for deployment to Hugging Face\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b58a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models/model_final directory and save all deployment artifacts\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "model_final_dir = \"./models/model_final\"\n",
    "os.makedirs(model_final_dir, exist_ok=True)\n",
    "\n",
    "print(\"Preparing model for deployment...\")\n",
    "print(f\"Saving to: {model_final_dir}\")\n",
    "\n",
    "# 1. Save the trained model and tokenizer\n",
    "print(\"\\n1. Saving trained model and tokenizer...\")\n",
    "model_artifacts_dir = os.path.join(model_final_dir, \"model_artifacts\")\n",
    "os.makedirs(model_artifacts_dir, exist_ok=True)\n",
    "\n",
    "# Copy trained model files\n",
    "if os.path.exists(output_dir):\n",
    "    for file in os.listdir(output_dir):\n",
    "        src = os.path.join(output_dir, file)\n",
    "        dst = os.path.join(model_artifacts_dir, file)\n",
    "        if os.path.isfile(src):\n",
    "            shutil.copy2(src, dst)\n",
    "    print(f\"Model files copied to {model_artifacts_dir}\")\n",
    "else:\n",
    "    print(f\"Model directory not found: {output_dir}\")\n",
    "\n",
    "# 2. Save label mappings and metadata\n",
    "print(\"\\n2. Saving label mappings and metadata...\")\n",
    "\n",
    "# Enhanced label mappings\n",
    "label_mappings = {\n",
    "    \"label2id\": label2id,\n",
    "    \"id2label\": {int(k): v for k, v in id2label.items()},\n",
    "    \"num_labels\": len(label2id),\n",
    "    \"entity_types\": sorted([label[2:] for label in label2id.keys() if label.startswith('B-')]),\n",
    "    \"all_labels\": list(label2id.keys())\n",
    "}\n",
    "\n",
    "with open(os.path.join(model_final_dir, \"label_mappings.json\"), 'w', encoding='utf-8') as f:\n",
    "    json.dump(label_mappings, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Model configuration for deployment\n",
    "model_config = {\n",
    "    \"model_name\": \"xlm-roberta-large\",\n",
    "    \"task\": \"token-classification\",\n",
    "    \"domain\": \"Indonesian Legal Documents\",\n",
    "    \"language\": \"Indonesian\",\n",
    "    \"max_length\": metadata['max_length'],\n",
    "    \"num_labels\": len(label2id),\n",
    "    \"model_size\": \"large\",\n",
    "    \"framework\": \"transformers\",\n",
    "    \"entity_types\": label_mappings[\"entity_types\"],\n",
    "    \"training_date\": datetime.now().isoformat(),\n",
    "    \"model_version\": \"1.0.0\",\n",
    "    \"performance\": {\n",
    "        \"validation_f1\": eval_results.get('eval_f1', 0) if 'eval_results' in locals() else 0,\n",
    "        \"test_f1\": f1 if 'f1' in locals() else 0\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(os.path.join(model_final_dir, \"model_config.json\"), 'w', encoding='utf-8') as f:\n",
    "    json.dump(model_config, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Label mappings saved\")\n",
    "print(f\"Model configuration saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1fa28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create preprocessing utilities for deployment\n",
    "print(\"\\n3. Creating preprocessing utilities...\")\n",
    "\n",
    "# Text preprocessing functions\n",
    "preprocessing_code = '''\n",
    "import re\n",
    "import string\n",
    "from typing import List, Dict, Tuple, Any\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import numpy as np\n",
    "\n",
    "class IndonesianLegalNERPreprocessor:\n",
    "    \"\"\"\n",
    "    Preprocessing utilities for Indonesian Legal NER model\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: str):\n",
    "        self.model_path = model_path\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.label_mappings = None\n",
    "        self.max_length = 512\n",
    "        \n",
    "    def load_model_and_mappings(self):\n",
    "        \"\"\"Load the trained model, tokenizer, and label mappings\"\"\"\n",
    "        try:\n",
    "            # Load tokenizer and model\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n",
    "            self.model = AutoModelForTokenClassification.from_pretrained(self.model_path)\n",
    "            \n",
    "            # Load label mappings\n",
    "            import json\n",
    "            import os\n",
    "            \n",
    "            mappings_path = os.path.join(os.path.dirname(self.model_path), \"label_mappings.json\")\n",
    "            with open(mappings_path, 'r', encoding='utf-8') as f:\n",
    "                self.label_mappings = json.load(f)\n",
    "            \n",
    "            # Set max length from model config\n",
    "            config_path = os.path.join(os.path.dirname(self.model_path), \"model_config.json\")\n",
    "            if os.path.exists(config_path):\n",
    "                with open(config_path, 'r', encoding='utf-8') as f:\n",
    "                    config = json.load(f)\n",
    "                    self.max_length = config.get('max_length', 512)\n",
    "            \n",
    "            print(f\"Model loaded from {self.model_path}\")\n",
    "            print(f\"Found {len(self.label_mappings['entity_types'])} entity types\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Basic text cleaning for Indonesian legal documents\"\"\"\n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\\\s+', ' ', text)\n",
    "        # Remove excessive punctuation\n",
    "        text = re.sub(r'[.]{3,}', '...', text)\n",
    "        # Clean up quotes\n",
    "        text = re.sub(r'[\"“”„‟]', '\"', text)\n",
    "        text = re.sub(r'[‘’]', \"'\", text)\n",
    "        # Remove control characters\n",
    "        text = re.sub(r'[\\\\x00-\\\\x08\\\\x0B\\\\x0C\\\\x0E-\\\\x1F\\\\x7F]', '', text)\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    def extract_text_from_pdf(self, pdf_path: str) -> str:\n",
    "        \"\"\"Extract text from PDF file\"\"\"\n",
    "        try:\n",
    "            import PyPDF2\n",
    "            \n",
    "            with open(pdf_path, 'rb') as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                text = \"\"\n",
    "                \n",
    "                for page in pdf_reader.pages:\n",
    "                    text += page.extract_text() + \"\\\\n\"\n",
    "                \n",
    "            return self.clean_text(text)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"error extracting PDF: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def tokenize_text(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"Tokenize text for NER prediction\"\"\"\n",
    "        if not self.tokenizer:\n",
    "            raise ValueError(\"Tokenizer not loaded. Call load_model_and_mappings() first.\")\n",
    "        \n",
    "        # Split into words (tokens)\n",
    "        words = text.split()\n",
    "        \n",
    "        # Tokenize\n",
    "        encoding = self.tokenizer(\n",
    "            words,\n",
    "            is_split_into_words=True,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'encoding': encoding,\n",
    "            'words': words,\n",
    "            'word_ids': encoding.word_ids()\n",
    "        }\n",
    "    \n",
    "    def predict_entities(self, text: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Predict named entities in text\"\"\"\n",
    "        if not self.model or not self.tokenizer:\n",
    "            raise ValueError(\"Model not loaded. Call load_model_and_mappings() first.\")\n",
    "        \n",
    "        # Tokenize\n",
    "        tokenized = self.tokenize_text(text)\n",
    "        encoding = tokenized['encoding']\n",
    "        words = tokenized['words']\n",
    "        word_ids = tokenized['word_ids']\n",
    "        \n",
    "        # Predict\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**encoding)\n",
    "            predictions = torch.argmax(outputs.logits, dim=2)\n",
    "        \n",
    "        # Convert predictions to labels\n",
    "        predicted_labels = []\n",
    "        for pred in predictions[0]:\n",
    "            label_id = pred.item()\n",
    "            label = self.label_mappings['id2label'][str(label_id)]\n",
    "            predicted_labels.append(label)\n",
    "        \n",
    "        # Extract entities\n",
    "        entities = self._extract_entities_from_predictions(\n",
    "            words, predicted_labels, word_ids\n",
    "        )\n",
    "        \n",
    "        return entities\n",
    "    \n",
    "    def _extract_entities_from_predictions(self, words: List[str], \n",
    "                                         labels: List[str], \n",
    "                                         word_ids: List[int]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Extract entities from predictions\"\"\"\n",
    "        entities = []\n",
    "        current_entity = []\n",
    "        current_label = None\n",
    "        current_start = None\n",
    "        \n",
    "        for i, (word_id, label) in enumerate(zip(word_ids, labels)):\n",
    "            if word_id is None:  # Special tokens\n",
    "                continue\n",
    "                \n",
    "            if word_id >= len(words):  # Safety check\n",
    "                continue\n",
    "                \n",
    "            word = words[word_id]\n",
    "            \n",
    "            if label.startswith('B-'):\n",
    "                # Save previous entity\n",
    "                if current_entity:\n",
    "                    entities.append({\n",
    "                        'text': ' '.join(current_entity),\n",
    "                        'label': current_label,\n",
    "                        'start_word': current_start,\n",
    "                        'end_word': current_start + len(current_entity) - 1\n",
    "                    })\n",
    "                \n",
    "                # Start new entity\n",
    "                current_entity = [word]\n",
    "                current_label = label[2:]\n",
    "                current_start = word_id\n",
    "                \n",
    "            elif label.startswith('I-') and current_label == label[2:]:\n",
    "                # Continue current entity\n",
    "                if word_id == current_start + len(current_entity):\n",
    "                    current_entity.append(word)\n",
    "                    \n",
    "            else:  # O label or different entity type\n",
    "                # Save previous entity\n",
    "                if current_entity:\n",
    "                    entities.append({\n",
    "                        'text': ' '.join(current_entity),\n",
    "                        'label': current_label,\n",
    "                        'start_word': current_start,\n",
    "                        'end_word': current_start + len(current_entity) - 1\n",
    "                    })\n",
    "                \n",
    "                current_entity = []\n",
    "                current_label = None\n",
    "                current_start = None\n",
    "        \n",
    "        # Save final entity\n",
    "        if current_entity:\n",
    "            entities.append({\n",
    "                'text': ' '.join(current_entity),\n",
    "                'label': current_label,\n",
    "                'start_word': current_start,\n",
    "                'end_word': current_start + len(current_entity) - 1\n",
    "            })\n",
    "        \n",
    "        return entities\n",
    "'''\n",
    "\n",
    "# Save preprocessing utilities\n",
    "with open(os.path.join(model_final_dir, \"preprocessing.py\"), 'w', encoding='utf-8') as f:\n",
    "    f.write(preprocessing_code)\n",
    "\n",
    "print(f\"✓ Preprocessing utilities saved to preprocessing.py\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
