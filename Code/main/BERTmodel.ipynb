{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d7019ca",
   "metadata": {},
   "source": [
    "# BERT Model for Named Entity Recognition (NER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0a6117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import re\n",
    "from seqeval.metrics import f1_score, precision_score, recall_score, classification_report\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d121d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('../../Datasets/FINAL/DATASET_BERT_CHUNKED.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe2436b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dataset information\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"\\nColumns:\", df.columns.tolist())\n",
    "print(\"\\nNull values:\\n\", df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72e6072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the unique labels\n",
    "unique_labels = set()\n",
    "for labels in df['labels'].str.split():\n",
    "    unique_labels.update(labels)\n",
    "unique_labels = sorted(list(unique_labels))\n",
    "print(f\"Unique labels: {unique_labels}\")\n",
    "\n",
    "# Create label to id mapping\n",
    "label_to_id = {label: i for i, label in enumerate(unique_labels)}\n",
    "id_to_label = {i: label for i, label in enumerate(unique_labels)}\n",
    "\n",
    "print(f\"\\nLabel to ID mapping: {label_to_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f895af16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset class\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        labels = self.labels[idx].split()\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "        \n",
    "        # Align the labels with tokens (handling word pieces)\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "        token_labels = []\n",
    "        \n",
    "        text_words = text.split()\n",
    "        \n",
    "        # Prepare word to token map\n",
    "        word_ids = []\n",
    "        current_word_idx = -1\n",
    "        \n",
    "        for token_idx, token in enumerate(tokens):\n",
    "            if token.startswith(\"##\"):\n",
    "                # This is a continuation of the previous word\n",
    "                word_ids.append(current_word_idx)\n",
    "            else:\n",
    "                # This is a new word\n",
    "                current_word_idx += 1\n",
    "                word_ids.append(current_word_idx)\n",
    "                \n",
    "            if current_word_idx >= len(labels):\n",
    "                break\n",
    "                \n",
    "        # Convert labels to IDs and align with tokens\n",
    "        label_ids = [-100] * self.max_len  # -100 is ignored by PyTorch loss functions\n",
    "        \n",
    "        # Add [CLS] token label\n",
    "        label_ids[0] = -100\n",
    "        \n",
    "        for token_idx, word_idx in enumerate(word_ids):\n",
    "            if word_idx < len(labels) and token_idx + 1 < self.max_len:\n",
    "                label_ids[token_idx + 1] = label_to_id[labels[word_idx]]\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': torch.tensor(label_ids, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9844dcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained tokenizer (using correct Indonesian BERT model)\n",
    "tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\n",
    "\n",
    "# Load base BERT model and add new classification head for our labels\n",
    "from transformers import BertConfig\n",
    "config = BertConfig.from_pretrained('indobenchmark/indobert-base-p1')\n",
    "config.num_labels = len(label_to_id)\n",
    "\n",
    "model = BertForTokenClassification.from_pretrained(\n",
    "    'indobenchmark/indobert-base-p1',\n",
    "    config=config,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Model will be trained for {len(label_to_id)} labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4b867f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "print(f\"Training set size: {train_df.shape[0]}\")\n",
    "print(f\"Test set size: {test_df.shape[0]}\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = NERDataset(\n",
    "    train_df['text'].tolist(),\n",
    "    train_df['labels'].tolist(),\n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "test_dataset = NERDataset(\n",
    "    test_df['text'].tolist(),\n",
    "    test_df['labels'].tolist(),\n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc97f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train(model, dataloader, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        progress_bar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "    \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea8ba49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            \n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=2)\n",
    "            \n",
    "            # Remove padding and ignored tokens\n",
    "            for i in range(labels.shape[0]):\n",
    "                true_seq = []\n",
    "                pred_seq = []\n",
    "                for j in range(labels.shape[1]):\n",
    "                    if labels[i, j] != -100:\n",
    "                        true_seq.append(id_to_label[labels[i, j].item()])\n",
    "                        pred_seq.append(id_to_label[predictions[i, j].item()])\n",
    "                \n",
    "                true_labels.append(true_seq)\n",
    "                predicted_labels.append(pred_seq)\n",
    "    \n",
    "    return true_labels, predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956e973c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    avg_loss = train(model, train_loader, optimizer, device, epoch)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Average loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210566f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "true_labels, predicted_labels = evaluate(model, test_loader, device)\n",
    "\n",
    "# Print performance metrics\n",
    "print(\"F1 Score:\", f1_score(true_labels, predicted_labels))\n",
    "print(\"Precision:\", precision_score(true_labels, predicted_labels))\n",
    "print(\"Recall:\", recall_score(true_labels, predicted_labels))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fbccd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model_save_path = './models/bert_ner_model'\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be0c03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict NER for new text\n",
    "def predict_ner(text, model, tokenizer, device):\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize the text\n",
    "    encoded_input = tokenizer(\n",
    "        text,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = encoded_input['input_ids'].to(device)\n",
    "    attention_mask = encoded_input['attention_mask'].to(device)\n",
    "    \n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=2)\n",
    "    \n",
    "    # Convert predictions to labels\n",
    "    predicted_labels = []\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    \n",
    "    for token, prediction in zip(tokens, predictions[0]):\n",
    "        if token in [tokenizer.cls_token, tokenizer.sep_token, tokenizer.pad_token]:\n",
    "            continue\n",
    "            \n",
    "        predicted_label = id_to_label[prediction.item()]\n",
    "        predicted_labels.append((token, predicted_label))\n",
    "    \n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd721019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a new document\n",
    "sample_text = \"\"\"\n",
    "PUTUSAN Nomor 123/Pid. B/2020/PN Jkt DEMI KEADILAN BERDASARKAN KETUHANAN YANG MAHA ESA\n",
    "Pengadilan Negeri Jakarta yang mengadili perkara pidana dengan acara pemeriksaan biasa dalam \n",
    "tingkat pertama menjatuhkan putusan sebagai berikut dalam perkara Terdakwa:\n",
    "Nama lengkap : Budi Santoso;\n",
    "Tempat lahir : Jakarta;\n",
    "Umur/tanggal lahir : 35 Tahun/10 Januari 1985;\n",
    "Jenis kelamin : Laki-laki;\n",
    "Kebangsaan : Indonesia;\n",
    "\"\"\"\n",
    "\n",
    "predicted_entities = predict_ner(sample_text, model, tokenizer, device)\n",
    "\n",
    "# Display results\n",
    "for token, label in predicted_entities:\n",
    "    if label != 'O':  # Only show named entities\n",
    "        print(f\"{token}: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b8dd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the entities in a more structured way\n",
    "from IPython.display import display, HTML\n",
    "import pandas as pd\n",
    "\n",
    "def visualize_entities(text, model, tokenizer, device):\n",
    "    # Get predictions\n",
    "    encoded_input = tokenizer(\n",
    "        text,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = encoded_input['input_ids'].to(device)\n",
    "    attention_mask = encoded_input['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "    \n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=2)\n",
    "    \n",
    "    # Get tokens and predictions\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    token_predictions = [id_to_label[p.item()] for p in predictions[0]]\n",
    "    \n",
    "    # Create a dataframe for visualization\n",
    "    df = pd.DataFrame({\n",
    "        'Token': tokens,\n",
    "        'Prediction': token_predictions\n",
    "    })\n",
    "    \n",
    "    # Filter out special tokens\n",
    "    special_tokens = [tokenizer.cls_token, tokenizer.sep_token, tokenizer.pad_token]\n",
    "    df = df[~df['Token'].isin(special_tokens)]\n",
    "    \n",
    "    # Only show entities (not O)\n",
    "    entity_df = df[df['Prediction'] != 'O']\n",
    "    \n",
    "    # Group entities\n",
    "    entities = []\n",
    "    current_entity = None\n",
    "    current_type = None\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        if row['Prediction'].startswith('B_'):\n",
    "            if current_entity:\n",
    "                entities.append((current_entity, current_type))\n",
    "            current_entity = row['Token']\n",
    "            current_type = row['Prediction'][2:]  # Remove B_ prefix\n",
    "        elif row['Prediction'].startswith('I_') and current_entity and row['Prediction'][2:] == current_type:\n",
    "            current_entity += \" \" + row['Token'].replace('##', '')\n",
    "        elif row['Prediction'] == 'O':\n",
    "            if current_entity:\n",
    "                entities.append((current_entity, current_type))\n",
    "                current_entity = None\n",
    "                current_type = None\n",
    "    \n",
    "    if current_entity:\n",
    "        entities.append((current_entity, current_type))\n",
    "    \n",
    "    # Create a result dataframe\n",
    "    result_df = pd.DataFrame(entities, columns=['Entity', 'Type'])\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# Test with sample text\n",
    "result = visualize_entities(sample_text, model, tokenizer, device)\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b2dbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a full document for testing\n",
    "def process_document(text, model, tokenizer, device, chunk_size=400):\n",
    "    words = text.split()\n",
    "    chunks = [' '.join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "    \n",
    "    all_entities = []\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        chunk_entities = visualize_entities(chunk, model, tokenizer, device)\n",
    "        all_entities.append(chunk_entities)\n",
    "    \n",
    "    # Combine results\n",
    "    if all_entities:\n",
    "        return pd.concat(all_entities, ignore_index=True)\n",
    "    else:\n",
    "        return pd.DataFrame(columns=['Entity', 'Type'])\n",
    "\n",
    "# Use a sample from the dataset\n",
    "document_text = df['text'].iloc[0]\n",
    "\n",
    "# Process the document\n",
    "document_entities = process_document(document_text, model, tokenizer, device)\n",
    "display(document_entities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
