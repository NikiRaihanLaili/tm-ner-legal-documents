{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d7019ca",
   "metadata": {},
   "source": [
    "# BERT Model for Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f0a6117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import re\n",
    "from seqeval.metrics import f1_score, precision_score, recall_score, classification_report\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d121d35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "labels",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "original_row_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "chunk_number",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "total_chunks",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "token_count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "008bf2b8-0c01-4fe3-b2d2-fa7651e9986a",
       "rows": [
        [
         "0",
         "PUTUSAN Nomor 192/Pid. B/2019/PN Bkl DEMI KEADILAN BERDASARKAN KETUHANAN YANG MAHA ESA Pengadilan Negeri Bangkalan yang mengadili perkara pidana dengan acara pemeriksaan biasa dalam tingkat pertama menjatuhkan putusan sebagai berikut dalam perkara Terdakwa: Nama lengkap : H. Abd Aziz Al. H. Aziz ;. Tempat lahir : Bangkalan;. Umur/tanggal lahir : 44 Tahun/5 Mei 1975;. Jenis kelamin: Laki-laki; . Kebangsaan : Indonesia;. Tempat tinggal: Dusun Duur Desa Langkap, Kecamatan Burneh,. Kabupaten Bangkalan; Agama : Islam;. Pekerjaan : Tidak bekerja; Dalam perkara ini, Terdakwa ditangkap oleh Penyidik dalam perkara lain; Terdakwa ditahan dalam perkara lain; Terdakwa menghadap sendiri dan tidak didampingi oleh Penasihat Hukum; Pengadilan Negeri tersebut; Setelah membaca : -Penetapan Ketua Pengadilan Negeri Bangkalan Nomor 192/Pid. B/2019/PN Bkl tanggal 16 Mei 2019 tentang Penunjukan Majelis Hakim yang mengadili perkara",
         "O O B_VERN I_VERN I_VERN O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B_DEFN I_DEFN I_DEFN I_DEFN I_DEFN I_DEFN O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B_VERN I_VERN I_VERN O O O O O O O O O O O",
         "0",
         "0",
         "88",
         "342"
        ],
        [
         "1",
         "Dalam perkara ini, Terdakwa ditangkap oleh Penyidik dalam perkara lain; Terdakwa ditahan dalam perkara lain; Terdakwa menghadap sendiri dan tidak didampingi oleh Penasihat Hukum; Pengadilan Negeri tersebut; Setelah membaca : -Penetapan Ketua Pengadilan Negeri Bangkalan Nomor 192/Pid. B/2019/PN Bkl tanggal 16 Mei 2019 tentang Penunjukan Majelis Hakim yang mengadili perkara ini; -Penetapan Majelis Hakim Nomor 192/Pid. B/2019/PN Bkl tanggal 20 Mei 2019 tentang penetapan hari sidang; -Berkas perkara dan surat-surat lain yang bersangkutan; Setelah mendengar keterangan Saksi-Saksi dan Terdakwa serta memperhatikan Alat Bukti dan barang bukti yang diajukan di persidangan; Setelah mendengar pembacaan tuntutan pidana yang diajukan oleh Penuntut Umum yang pada pokoknya sebagai berikut: 1. Menyatakan Terdakwa H. Abd Aziz Al. H. Aziz secara sah dan menyakinkan terbukti bersalah melakukan tindak pidana “Pencurian diancam pidana dalam ketentuan",
         "O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B_VERN I_VERN I_VERN O O O O O O O O O O O O O O O O B_VERN I_VERN I_VERN O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B_DEFN I_DEFN I_DEFN I_DEFN I_DEFN I_DEFN O O O O O O O O O O O O O O",
         "0",
         "1",
         "88",
         "342"
        ],
        [
         "2",
         "Terdakwa serta memperhatikan Alat Bukti dan barang bukti yang diajukan di persidangan; Setelah mendengar pembacaan tuntutan pidana yang diajukan oleh Penuntut Umum yang pada pokoknya sebagai berikut: 1. Menyatakan Terdakwa H. Abd Aziz Al. H. Aziz secara sah dan menyakinkan terbukti bersalah melakukan tindak pidana “Pencurian diancam pidana dalam ketentuan Pasal 365 Ayat (2) ke-1 Kitab Undang- Undang Hukum Pidana dalam dakwaan Primair; 2. Menjatuhkan pidana terhadap Terdakwa H. Abd Aziz Al. H. Aziz dengan pidana penjara selama 6 (enam) tahun dan 6 (enam) bulan dikurangi selama Terdakwa berada dalam tahanan dan dengan perintah tetap ditahan; 3. Menyatakan Barang bukti berupa: -1 (satu) buah Dusbook Handphone Samsung Galaxi A6 warna hitam dengan No. IMEI 357931/09/412666/6, IMEI 357932/09 / 412666/4; -1 (satu) buah Handphone Samsung Galaxi A6 warna hitam",
         "O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B_DEFN I_DEFN I_DEFN I_DEFN I_DEFN I_DEFN O O O O O O O O O O O O O O B_ARTV I_ARTV I_ARTV I_ARTV I_ARTV I_ARTV O O O O O O O O O O O O B_DEFN I_DEFN I_DEFN I_DEFN I_DEFN I_DEFN O O O O B_PENA I_PENA I_PENA O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O",
         "0",
         "2",
         "88",
         "326"
        ],
        [
         "3",
         "6 (enam) tahun dan 6 (enam) bulan dikurangi selama Terdakwa berada dalam tahanan dan dengan perintah tetap ditahan; 3. Menyatakan Barang bukti berupa: -1 (satu) buah Dusbook Handphone Samsung Galaxi A6 warna hitam dengan No. IMEI 357931/09/412666/6, IMEI 357932/09 / 412666/4; -1 (satu) buah Handphone Samsung Galaxi A6 warna hitam dengan No. IMEI 357931/09/412666/6, No. IMEI 357932/09/412666/4; Dikembalikan Kepada Pemiliknya yaitu Saksi Lenny Marethasari ; 4. Menetapkan supaya Terdakwa dibebani biaya perkara masing-masing sejumlah Rp2. 000,00 (dua ribu rupiah); Setelah mendengar permohonan Terdakwa yang pada pokoknya menyatakan meminta keringanan atas tuntutan yang diajukan oleh Penuntut Umum; Menimbang, bahwa Terdakwa diajukan ke persidangan oleh Penuntut Umum didakwa berdasarkan surat dakwaan No. Reg. Perkara PDM-56/0. 5. 37 /Euh. 2/04/ 2019 tanggal 2 Mei 2019 sebagai berikut: PRIMAIR Bahwa ia Terdakwa",
         "B_PENA I_PENA I_PENA O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O",
         "0",
         "3",
         "88",
         "367"
        ],
        [
         "4",
         "rupiah); Setelah mendengar permohonan Terdakwa yang pada pokoknya menyatakan meminta keringanan atas tuntutan yang diajukan oleh Penuntut Umum; Menimbang, bahwa Terdakwa diajukan ke persidangan oleh Penuntut Umum didakwa berdasarkan surat dakwaan No. Reg. Perkara PDM-56/0. 5. 37 /Euh. 2/04/ 2019 tanggal 2 Mei 2019 sebagai berikut: PRIMAIR Bahwa ia Terdakwa H. Abd Aziz Al. H. Aziz , pada hari Sabtu tanggal 15 Desember 2018 sekitar pukul 14. 00 WIB atau di suatu waktu pada bulan Desember 2018 atau setidak -tidaknya pada suatu waktu yang masih dalam tahun 2018, bertempat di depan SPBU Jalan Raya Tangkel Desa Burneh Kecamatan Burneh Kab upaten Bangkalan atau setidak-tidaknya di suatu tempat yang masih dalam wilayah hukum Pengadilan Negeri Bangkalan yang berwenang memeriksa dan mengadili perkara ini, “ Terdakwa melakukan tindak pidana pencurian",
         "O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B_DEFN I_DEFN I_DEFN I_DEFN I_DEFN I_DEFN O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O",
         "0",
         "4",
         "88",
         "315"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>original_row_id</th>\n",
       "      <th>chunk_number</th>\n",
       "      <th>total_chunks</th>\n",
       "      <th>token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PUTUSAN Nomor 192/Pid. B/2019/PN Bkl DEMI KEAD...</td>\n",
       "      <td>O O B_VERN I_VERN I_VERN O O O O O O O O O O O...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "      <td>342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dalam perkara ini, Terdakwa ditangkap oleh Pen...</td>\n",
       "      <td>O O O O O O O O O O O O O O O O O O O O O O O ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>88</td>\n",
       "      <td>342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Terdakwa serta memperhatikan Alat Bukti dan ba...</td>\n",
       "      <td>O O O O O O O O O O O O O O O O O O O O O O O ...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>88</td>\n",
       "      <td>326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6 (enam) tahun dan 6 (enam) bulan dikurangi se...</td>\n",
       "      <td>B_PENA I_PENA I_PENA O O O O O O O O O O O O O...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>88</td>\n",
       "      <td>367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rupiah); Setelah mendengar permohonan Terdakwa...</td>\n",
       "      <td>O O O O O O O O O O O O O O O O O O O O O O O ...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>88</td>\n",
       "      <td>315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  PUTUSAN Nomor 192/Pid. B/2019/PN Bkl DEMI KEAD...   \n",
       "1  Dalam perkara ini, Terdakwa ditangkap oleh Pen...   \n",
       "2  Terdakwa serta memperhatikan Alat Bukti dan ba...   \n",
       "3  6 (enam) tahun dan 6 (enam) bulan dikurangi se...   \n",
       "4  rupiah); Setelah mendengar permohonan Terdakwa...   \n",
       "\n",
       "                                              labels  original_row_id  \\\n",
       "0  O O B_VERN I_VERN I_VERN O O O O O O O O O O O...                0   \n",
       "1  O O O O O O O O O O O O O O O O O O O O O O O ...                0   \n",
       "2  O O O O O O O O O O O O O O O O O O O O O O O ...                0   \n",
       "3  B_PENA I_PENA I_PENA O O O O O O O O O O O O O...                0   \n",
       "4  O O O O O O O O O O O O O O O O O O O O O O O ...                0   \n",
       "\n",
       "   chunk_number  total_chunks  token_count  \n",
       "0             0            88          342  \n",
       "1             1            88          342  \n",
       "2             2            88          326  \n",
       "3             3            88          367  \n",
       "4             4            88          315  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('../../Datasets/FINAL/DATASET_BERT_CHUNKED.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fe2436b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (4930, 6)\n",
      "\n",
      "Columns: ['text', 'labels', 'original_row_id', 'chunk_number', 'total_chunks', 'token_count']\n",
      "\n",
      "Null values:\n",
      " text               0\n",
      "labels             0\n",
      "original_row_id    0\n",
      "chunk_number       0\n",
      "total_chunks       0\n",
      "token_count        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check dataset information\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"\\nColumns:\", df.columns.tolist())\n",
    "print(\"\\nNull values:\\n\", df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e72e6072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels: ['B_ARTV', 'B_CRIA', 'B_DEFN', 'B_JUDG', 'B_JUDP', 'B_PENA', 'B_PROS', 'B_PUNI', 'B_REGI', 'B_TIMV', 'B_VERN', 'I_ARTV', 'I_CRIA', 'I_DEFN', 'I_JUDG', 'I_JUDP', 'I_PENA', 'I_PROS', 'I_PUNI', 'I_REGI', 'I_TIMV', 'I_VERN', 'O']\n",
      "\n",
      "Label to ID mapping: {'B_ARTV': 0, 'B_CRIA': 1, 'B_DEFN': 2, 'B_JUDG': 3, 'B_JUDP': 4, 'B_PENA': 5, 'B_PROS': 6, 'B_PUNI': 7, 'B_REGI': 8, 'B_TIMV': 9, 'B_VERN': 10, 'I_ARTV': 11, 'I_CRIA': 12, 'I_DEFN': 13, 'I_JUDG': 14, 'I_JUDP': 15, 'I_PENA': 16, 'I_PROS': 17, 'I_PUNI': 18, 'I_REGI': 19, 'I_TIMV': 20, 'I_VERN': 21, 'O': 22}\n"
     ]
    }
   ],
   "source": [
    "# Process the unique labels\n",
    "unique_labels = set()\n",
    "for labels in df['labels'].str.split():\n",
    "    unique_labels.update(labels)\n",
    "unique_labels = sorted(list(unique_labels))\n",
    "print(f\"Unique labels: {unique_labels}\")\n",
    "\n",
    "# Create label to id mapping\n",
    "label_to_id = {label: i for i, label in enumerate(unique_labels)}\n",
    "id_to_label = {i: label for i, label in enumerate(unique_labels)}\n",
    "\n",
    "print(f\"\\nLabel to ID mapping: {label_to_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f895af16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset class\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        labels = self.labels[idx].split()\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "        \n",
    "        # Align the labels with tokens (handling word pieces)\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "        token_labels = []\n",
    "        \n",
    "        text_words = text.split()\n",
    "        \n",
    "        # Prepare word to token map\n",
    "        word_ids = []\n",
    "        current_word_idx = -1\n",
    "        \n",
    "        for token_idx, token in enumerate(tokens):\n",
    "            if token.startswith(\"##\"):\n",
    "                # This is a continuation of the previous word\n",
    "                word_ids.append(current_word_idx)\n",
    "            else:\n",
    "                # This is a new word\n",
    "                current_word_idx += 1\n",
    "                word_ids.append(current_word_idx)\n",
    "                \n",
    "            if current_word_idx >= len(labels):\n",
    "                break\n",
    "                \n",
    "        # Convert labels to IDs and align with tokens\n",
    "        label_ids = [-100] * self.max_len  # -100 is ignored by PyTorch loss functions\n",
    "        \n",
    "        # Add [CLS] token label\n",
    "        label_ids[0] = -100\n",
    "        \n",
    "        for token_idx, word_idx in enumerate(word_ids):\n",
    "            if word_idx < len(labels) and token_idx + 1 < self.max_len:\n",
    "                label_ids[token_idx + 1] = label_to_id[labels[word_idx]]\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': torch.tensor(label_ids, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9844dcfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26ac68d4a62e437ba9bf30bdc348586b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28068f6115ce47518a0f983c58dfe72a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/234k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbc88a105c2d44f9bccfa1b3eb7cf080",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6821d019554c4d4daa7b21964c513bbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "646b6afa93194905a1ca03bf6ad847de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.01k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ac294d2a33c4bae90d21186600c83e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/445M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at indolem/indobert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('indolem/indobert-base-uncased')\n",
    "model = BertForTokenClassification.from_pretrained(\n",
    "    'indolem/indobert-base-uncased',\n",
    "    num_labels=len(label_to_id)\n",
    ")\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb4b867f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 3944\n",
      "Test set size: 986\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "print(f\"Training set size: {train_df.shape[0]}\")\n",
    "print(f\"Test set size: {test_df.shape[0]}\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = NERDataset(\n",
    "    train_df['text'].tolist(),\n",
    "    train_df['labels'].tolist(),\n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "test_dataset = NERDataset(\n",
    "    test_df['text'].tolist(),\n",
    "    test_df['labels'].tolist(),\n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecc97f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train(model, dataloader, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        progress_bar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "    \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cea8ba49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            \n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=2)\n",
    "            \n",
    "            # Remove padding and ignored tokens\n",
    "            for i in range(labels.shape[0]):\n",
    "                true_seq = []\n",
    "                pred_seq = []\n",
    "                for j in range(labels.shape[1]):\n",
    "                    if labels[i, j] != -100:\n",
    "                        true_seq.append(id_to_label[labels[i, j].item()])\n",
    "                        pred_seq.append(id_to_label[predictions[i, j].item()])\n",
    "                \n",
    "                true_labels.append(true_seq)\n",
    "                predicted_labels.append(pred_seq)\n",
    "    \n",
    "    return true_labels, predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "956e973c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   1%|          | 5/493 [00:12<21:44,  2.67s/it, loss=0.5017]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f79ac591623490698eb10912377e0eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/445M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 493/493 [20:46<00:00,  2.53s/it, loss=0.2563]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Average loss: 0.3412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 493/493 [20:28<00:00,  2.49s/it, loss=0.1789]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Average loss: 0.2134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 493/493 [20:35<00:00,  2.51s/it, loss=0.1074]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Average loss: 0.1826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    avg_loss = train(model, train_loader, optimizer, device, epoch)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Average loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "210566f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 124/124 [01:14<00:00,  1.66it/s]\n",
      "e:\\SEMESTER 6\\TEXT MINING\\Uas\\.venv\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: I_DEFN seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\SEMESTER 6\\TEXT MINING\\Uas\\.venv\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: B_PENA seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\SEMESTER 6\\TEXT MINING\\Uas\\.venv\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: I_PENA seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\SEMESTER 6\\TEXT MINING\\Uas\\.venv\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: B_ARTV seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\SEMESTER 6\\TEXT MINING\\Uas\\.venv\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: I_ARTV seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\SEMESTER 6\\TEXT MINING\\Uas\\.venv\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: B_CRIA seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\SEMESTER 6\\TEXT MINING\\Uas\\.venv\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: B_DEFN seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\SEMESTER 6\\TEXT MINING\\Uas\\.venv\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: B_REGI seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\SEMESTER 6\\TEXT MINING\\Uas\\.venv\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: I_REGI seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\SEMESTER 6\\TEXT MINING\\Uas\\.venv\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: B_PROS seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\SEMESTER 6\\TEXT MINING\\Uas\\.venv\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: I_PROS seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\SEMESTER 6\\TEXT MINING\\Uas\\.venv\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: B_PUNI seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\SEMESTER 6\\TEXT MINING\\Uas\\.venv\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: I_PUNI seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\SEMESTER 6\\TEXT MINING\\Uas\\.venv\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: B_TIMV seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\SEMESTER 6\\TEXT MINING\\Uas\\.venv\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: I_TIMV seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\SEMESTER 6\\TEXT MINING\\Uas\\.venv\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: B_JUDP seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\SEMESTER 6\\TEXT MINING\\Uas\\.venv\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: I_JUDP seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\SEMESTER 6\\TEXT MINING\\Uas\\.venv\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: I_CRIA seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\SEMESTER 6\\TEXT MINING\\Uas\\.venv\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: B_VERN seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\SEMESTER 6\\TEXT MINING\\Uas\\.venv\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: I_VERN seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\SEMESTER 6\\TEXT MINING\\Uas\\.venv\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: B_JUDG seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "e:\\SEMESTER 6\\TEXT MINING\\Uas\\.venv\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: I_JUDG seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.09770992366412214\n",
      "Precision: 0.15194681861348527\n",
      "Recall: 0.07200720072007201\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       _ARTV       0.00      0.00      0.00       252\n",
      "       _CRIA       0.09      0.03      0.05       120\n",
      "       _DEFN       0.21      0.09      0.12       927\n",
      "       _JUDG       0.00      0.00      0.00       181\n",
      "       _JUDP       0.05      0.04      0.04       113\n",
      "       _PENA       0.03      0.03      0.03        67\n",
      "       _PROS       0.06      0.02      0.03        89\n",
      "       _PUNI       0.20      0.10      0.13        93\n",
      "       _REGI       0.03      0.03      0.03       109\n",
      "       _TIMV       0.08      0.03      0.04        63\n",
      "       _VERN       0.55      0.25      0.34       208\n",
      "\n",
      "   micro avg       0.15      0.07      0.10      2222\n",
      "   macro avg       0.12      0.06      0.07      2222\n",
      "weighted avg       0.16      0.07      0.10      2222\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "true_labels, predicted_labels = evaluate(model, test_loader, device)\n",
    "\n",
    "# Print performance metrics\n",
    "print(\"F1 Score:\", f1_score(true_labels, predicted_labels))\n",
    "print(\"Precision:\", precision_score(true_labels, predicted_labels))\n",
    "print(\"Recall:\", recall_score(true_labels, predicted_labels))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5fbccd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./models/bert_ner_model\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "model_save_path = './models/bert_ner_model'\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9be0c03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict NER for new text\n",
    "def predict_ner(text, model, tokenizer, device):\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize the text\n",
    "    encoded_input = tokenizer(\n",
    "        text,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = encoded_input['input_ids'].to(device)\n",
    "    attention_mask = encoded_input['attention_mask'].to(device)\n",
    "    \n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=2)\n",
    "    \n",
    "    # Convert predictions to labels\n",
    "    predicted_labels = []\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    \n",
    "    for token, prediction in zip(tokens, predictions[0]):\n",
    "        if token in [tokenizer.cls_token, tokenizer.sep_token, tokenizer.pad_token]:\n",
    "            continue\n",
    "            \n",
    "        predicted_label = id_to_label[prediction.item()]\n",
    "        predicted_labels.append((token, predicted_label))\n",
    "    \n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd721019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123: B_VERN\n",
      "/: I_VERN\n",
      "pid: I_VERN\n",
      "dalam: B_DEFN\n"
     ]
    }
   ],
   "source": [
    "# Test with a new document\n",
    "sample_text = \"\"\"\n",
    "PUTUSAN Nomor 123/Pid. B/2020/PN Jkt DEMI KEADILAN BERDASARKAN KETUHANAN YANG MAHA ESA\n",
    "Pengadilan Negeri Jakarta yang mengadili perkara pidana dengan acara pemeriksaan biasa dalam \n",
    "tingkat pertama menjatuhkan putusan sebagai berikut dalam perkara Terdakwa:\n",
    "Nama lengkap : Budi Santoso;\n",
    "Tempat lahir : Jakarta;\n",
    "Umur/tanggal lahir : 35 Tahun/10 Januari 1985;\n",
    "Jenis kelamin : Laki-laki;\n",
    "Kebangsaan : Indonesia;\n",
    "\"\"\"\n",
    "\n",
    "predicted_entities = predict_ner(sample_text, model, tokenizer, device)\n",
    "\n",
    "# Display results\n",
    "for token, label in predicted_entities:\n",
    "    if label != 'O':  # Only show named entities\n",
    "        print(f\"{token}: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71b8dd3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Entity",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Type",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "4ba987d9-de72-4d67-98bf-98f35b22420c",
       "rows": [
        [
         "0",
         "123 / pid",
         "VERN"
        ],
        [
         "1",
         "dalam",
         "DEFN"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 2
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entity</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123 / pid</td>\n",
       "      <td>VERN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dalam</td>\n",
       "      <td>DEFN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Entity  Type\n",
       "0  123 / pid  VERN\n",
       "1      dalam  DEFN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the entities in a more structured way\n",
    "from IPython.display import display, HTML\n",
    "import pandas as pd\n",
    "\n",
    "def visualize_entities(text, model, tokenizer, device):\n",
    "    # Get predictions\n",
    "    encoded_input = tokenizer(\n",
    "        text,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = encoded_input['input_ids'].to(device)\n",
    "    attention_mask = encoded_input['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "    \n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=2)\n",
    "    \n",
    "    # Get tokens and predictions\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    token_predictions = [id_to_label[p.item()] for p in predictions[0]]\n",
    "    \n",
    "    # Create a dataframe for visualization\n",
    "    df = pd.DataFrame({\n",
    "        'Token': tokens,\n",
    "        'Prediction': token_predictions\n",
    "    })\n",
    "    \n",
    "    # Filter out special tokens\n",
    "    special_tokens = [tokenizer.cls_token, tokenizer.sep_token, tokenizer.pad_token]\n",
    "    df = df[~df['Token'].isin(special_tokens)]\n",
    "    \n",
    "    # Only show entities (not O)\n",
    "    entity_df = df[df['Prediction'] != 'O']\n",
    "    \n",
    "    # Group entities\n",
    "    entities = []\n",
    "    current_entity = None\n",
    "    current_type = None\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        if row['Prediction'].startswith('B_'):\n",
    "            if current_entity:\n",
    "                entities.append((current_entity, current_type))\n",
    "            current_entity = row['Token']\n",
    "            current_type = row['Prediction'][2:]  # Remove B_ prefix\n",
    "        elif row['Prediction'].startswith('I_') and current_entity and row['Prediction'][2:] == current_type:\n",
    "            current_entity += \" \" + row['Token'].replace('##', '')\n",
    "        elif row['Prediction'] == 'O':\n",
    "            if current_entity:\n",
    "                entities.append((current_entity, current_type))\n",
    "                current_entity = None\n",
    "                current_type = None\n",
    "    \n",
    "    if current_entity:\n",
    "        entities.append((current_entity, current_type))\n",
    "    \n",
    "    # Create a result dataframe\n",
    "    result_df = pd.DataFrame(entities, columns=['Entity', 'Type'])\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# Test with sample text\n",
    "result = visualize_entities(sample_text, model, tokenizer, device)\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57b2dbdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Entity",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Type",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "6f9518de-957c-4a90-bb3a-1b376367d126",
       "rows": [
        [
         "0",
         "192 / pid",
         "VERN"
        ],
        [
         "1",
         "dalam perkara terdakwa : nama",
         "DEFN"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 2
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entity</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>192 / pid</td>\n",
       "      <td>VERN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dalam perkara terdakwa : nama</td>\n",
       "      <td>DEFN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Entity  Type\n",
       "0                      192 / pid  VERN\n",
       "1  dalam perkara terdakwa : nama  DEFN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load a full document for testing\n",
    "def process_document(text, model, tokenizer, device, chunk_size=400):\n",
    "    words = text.split()\n",
    "    chunks = [' '.join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "    \n",
    "    all_entities = []\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        chunk_entities = visualize_entities(chunk, model, tokenizer, device)\n",
    "        all_entities.append(chunk_entities)\n",
    "    \n",
    "    # Combine results\n",
    "    if all_entities:\n",
    "        return pd.concat(all_entities, ignore_index=True)\n",
    "    else:\n",
    "        return pd.DataFrame(columns=['Entity', 'Type'])\n",
    "\n",
    "# Use a sample from the dataset\n",
    "document_text = df['text'].iloc[0]\n",
    "\n",
    "# Process the document\n",
    "document_entities = process_document(document_text, model, tokenizer, device)\n",
    "display(document_entities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
